\documentclass[11pt]{report}
\usepackage[a4paper]{geometry}
\usepackage{outline}
\usepackage{pmgraph}
\usepackage[normalem]{ulem}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{eucal}
\usepackage{amssymb}
\usepackage{mathrsfs}
\usepackage{tikz}
\usepackage{pgf}
\usepackage{lscape}
\usetikzlibrary{arrows,automata}
\usepackage{verbatim}
\usepackage{float}
\usepackage[titletoc]{appendix}
\allowdisplaybreaks
\usepackage{listings}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{url}
\usepackage{color, colortbl}
\usepackage{nicefrac}
\usepackage[intoc]{nomencl}
\lstset{language=matlab,frame=single}
\usepackage[framed,numbered,autolinebreaks,useliterate]{mcode}
\setcounter{secnumdepth}{5}
\setcounter{tocdepth}{4}
\usepackage{tocloft}
\usepackage{blkarray}
\usepackage{placeins}
\usepackage{tkz-berge}
\thispagestyle{empty}
\usetikzlibrary{fit,shapes}
\usepackage{import}
\usepackage{lipsum}
\newtheorem{theorem}{Theorem}[chapter]
\usepackage{nth}
\usepackage{paralist}
\newtheorem*{theorem*}{Theorem}
\renewcommand{\bibname}{References}




\makenomenclature
\renewcommand{\nomname}{List of Symbols and Abbreviations}

\usepackage{ifthen}
  \renewcommand{\nomgroup}[1]{%
  \item[\bfseries
  \ifthenelse{\equal{#1}{A}}{Abbreviations}{%
  \ifthenelse{\equal{#1}{B}}{PageRank Notation}{%
  \ifthenelse{\equal{#1}{C}}{HITS Notation}{%
  \ifthenelse{\equal{#1}{D}}{SALSA Notation}{}}}}%
  ]}

\title{\textbf{PageRank and Random Walks on the Web}}
\author{Shannon Chatha}
\date{\today}

\begin{document}
%--------------------Title Page
\maketitle
\begin{abstract}
This report explores the PageRank algorithm as a method of ranking web pages by importance. It outlines the mathematical representation of the algorithm and explores the use of the power method to produce a ranked list. The report explores other methods of ranking web pages such as the HITS and SALSA algorithms and discusses the advantages and disadvantages of the three algorithms. Further improvements that can improve the standard PageRank algorithm are outlined. Finally this report looks at applications of the PageRank algorithm beyond ranking web pages, and focuses on the use of the algorithm in mapping the road network of Durham with the aim of predicting human movement in the city.
\end{abstract}
\newpage
\vspace*{\fill}
\begin{center}
\textbf{Declaration of Authorship}\\ 
\textit{This piece of work is a result of my own work except where it forms an assessment
based on group project work. In the case of a group project, the work
has been prepared in collaboration with other members of the group. Material
from the work of others not involved in the project has been acknowledged and
quotations and paraphrases suitably indicated.}
\bigbreak
\bigbreak
\textbf{Acknowledgements}

\end{center}
\vspace*{\fill}

\mbox{}
\nomenclature[01]{$r(P_i)$}{Rank of page \textit{i}}
\nomenclature[02]{$\boldsymbol{\pi}$}{PageRank vector}
 
\nomenclature[A,01]{PR}{PageRank}
\nomenclature[A,02]{HITS}{Hypertext Induced Topic Search}
\nomenclature[A,03]{SALSA}{Stochastic Approach to Linear Systems Analysis}
\nomenclature[A,04]{TKC}{Tightly Knitted Community}
\nomenclature[A,05]{QDPR}{Query-Dependent PageRank}
\nomenclature[A,06]{TSPR}{Topic-Sensitive PageRank}
\nomenclature[A,07]{WPR}{Weighted PageRank}

\nomenclature[B,01]{\textbf{H}}{Hyperlink Matrix}
\nomenclature[B,02]{\textbf{S}}{Stochastic Matrix}
\nomenclature[B,03]{$1^T$}{Rank 1 matrix with unit entries of size $[1 \times n]$}
\nomenclature[B,04]{\textbf{a}}{Dangling node vector}
\nomenclature[B,05]{\textbf{G}}{Google Matrix}
\nomenclature[B,06]{$\alpha$}{Scaling parameter $\in$ [0,1]}
\nomenclature[B,07]{$\textbf{v}^T$}{Personalization vector}

\nomenclature[C,01]{N}{Neighbourhood graph}
\nomenclature[C,02]{\textbf{L}}{Adjacency Matrix}
\nomenclature[C,03]{\textbf{e}}{Rank 1 matrix with unit entries of size $[1 \times n]$}
\nomenclature[C,04]{$\textbf{L}^T\textbf{L}$}{Authority Matrix}
\nomenclature[C,05]{$\textbf{LL}^T$}{Hub Matrix}
\nomenclature[C,06]{\textbf{x}}{Authority vector}
\nomenclature[C,07]{\textbf{y}}{Hub vector}

\nomenclature[D,01]{N}{Neighbourhood graph}
\nomenclature[D,02]{G}{Bipartite undirected graph}
\nomenclature[D,03]{$\textbf{L}_r$}{\textbf{L} with each non-zero row divided by row sum}
\nomenclature[D,04]{$\textbf{L}_c$}{\textbf{L} with each non-zero column divided by column sum}
\nomenclature[D,05]{\textbf{A}}{Authority Matrix}
\nomenclature[D,06]{\textbf{H}}{Hub Matrix}
\nomenclature[D,08]{$\boldsymbol{\pi}_h^T$}{Hub vector}
\nomenclature[D,07]{$\boldsymbol{\pi}_a^T$}{Authority vector}

\tableofcontents
\newpage
\listoffigures
\listoftables
\addcontentsline{toc}{chapter}{List of Figures}
\addcontentsline{toc}{chapter}{List of Tables}
\printnomenclature[5em]
%--------------------Introduction
\chapter{Introduction}\label{chap:import}
Google's PageRank algorithm, developed in 1998 by Sergey Brin and Larry Page is one of the most well-known and influential methods of ranking web pages. The hyperlink structure of the web is represented by a directed graph, with the web pages representing the nodes and the links between nodes being the hyperlinks. Inlinks are pages that point into nodes, and outlinks point out from nodes \cite{brin1998anatomy}. The algorithm in its simplest form aims to rank pages with the assumption that a web page's importance is reliant on other important pages linking to it. When this is represented mathematically, it becomes clear that the importance scores of the web pages relate to the stationary points of a Markov chain, and hence Markov theory and matrices can be used to solve the PageRank equation:
\begin{equation} \label{eq:PR}
r(P_i) = (1-\alpha) + \alpha \displaystyle \sum_{P_j\in B_{P_i }} \frac{r(P_j)}{|P_j|}
\end{equation} where $r(P_i)$ is the rank of page \textit{i}, $\alpha$ is a scaling parameter $\in [0,1]$, typically set at 0.85. We are now able to use the power method in order to solve this equation, 
\[\boldsymbol{\pi} = \boldsymbol{\pi} \cdot \textbf{G}\] 
where \textbf{G} is the `Google Matrix', as defined in Section \ref{sec:google}, and $\boldsymbol{\pi}$ is the set of ranked web pages \cite{langville}.
We are able to interpret a web pages PageRank as the fraction of time that a random surfer spends on that web page, with the random surfer more likely to return to the most important pages \cite{bonato}. Throughout this report we will be using the network graph as an example: 
\begin{figure}[H]
\centering
\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=2cm,
                    semithick]
\tikzstyle{every state}=[fill=white,draw=black,text=black]
\node[state] (1) {$1$};
\node[state] (2) [right of=1] {$2$};
\node[state] (3) [right of=2] {$3$};
\node[state] (4) [below of=1] {$4$};
\node[state] (5) [below of=3] {$5$};
\node[state] (6) [below of=4] {$6$};
\node[state] (7) [right of=6] {$7$};
\node[state] (8) [right of=7] {$8$};
\path (1) edge node{} (2)
          edge node{} (4)
          edge [bend right] node{} (6)
      (2) edge node{} (3)
          edge node{} (5)
      (3) edge node{} (5)
      (4) edge node{} (7)
          edge node{} (5)
      (5) edge [bend right] node{} (8)
      (6) edge node{} (4)
          edge node{} (7)
          edge [bend right] node{} (8)
      (8) edge [bend right] node{} (5)
          edge node{} (7);
          
\end{tikzpicture}
\caption{Graph modelling 8-node web} \label{fig:Example}
\end{figure}

In the first chapter of this report, we will explore mathematical representations of the PageRank equation, the modifications required on the matrix in order to use Markov theory to solve the PageRank equation, and finally on the use of the power method to solve the equation. Chapter \ref{chap:Other} discusses other algorithms that we could use to rank web pages, and compares these to PageRank. Chapter \ref{chap:Improve} will then explore improvements to the PageRank representation that could increase the accuracy and personalise the PageRank method for a user, such as Topic-Sensitive PageRank and Weighted PageRank. Chapter \ref{chap:Applications} explores potential applications for PageRank beyond ranking web-pages, for example in predicting traffic flow in Durham. The final chapter of the report, Chapter \ref{chap:Conclusion}, summarises the main points of the report.

%--------------------Matrix Representation 
\chapter{Mathematical Representation} \label{chap:Math}
In this chapter we aim to represent the PageRank equation mathematically, in Section \ref{sec:summ}, we use summation formulas to represent the algorithm, and introduce the idea of an iterative process to compute the PageRank vector. In Section \ref{sec:matrix} we represent the PageRank equation using matrices, and discuss the two adjustments required in order to use Markov theory to solve the PageRank equation leading to the formation of a dense Google Matrix. We will discuss the use of the power method to solve the PageRank equation in Section \ref{sec:solve}.

The references Austin \cite{austin}, Bonato \cite{bonato} and Langville \cite{langville} were consulted during the writing of this chapter, and the material has been collated below.  
\section{Summation Formula of PageRank} \label{sec:summ}
In order to mathematically represent the PageRank algorithm, Brin and Page first assigned each page, $P_i$, a rank
\[ r(P_i) = \displaystyle \sum_{P_j\in B_{P_i }} \frac{r(P_j)}{|P_j|} \]
where $B_{P_i}$ is the set of pages pointing into $P_i$, and $\vert P_j\vert$ is the number of outlinks from a page. However the ranking of outlinks is unknown, and so an iterative summation formula is required, where it is assumed that all pages have equal PageRank at the start, \(r_o = \frac{1}{n}\) , 
\begin{equation} \label{eq:iterative}
r_{(k+1)}P_i = \displaystyle \sum_{P_j\in B_{P_i }}\frac{r_k(P_j)}{|P_j|}
\end{equation} 
When Equation \eqref{eq:iterative} is applied to all pages in the graph, you can calculate a ranking for the pages, so for the graph in Figure \ref{fig:Example}, we get the following values for the PageRanks after some iterations: 

\begin{table}[H] \caption{First iterates using Equation \eqref{eq:iterative} on Figure \ref{fig:Example}}
 \centering
 \begin{tabular} {c c c |c} 
 Iter. 0 & Iter. 1 & Iter. 2 &  Rank at Iter. 2 \\ [0.5ex] 
 \hline
 $r_0(P_1)=\frac{1}{8}$ & $r_1(P_1)=0$ & $r_2(P_1)=0$ & $6$ \\ 
 $r_0(P_2)=\frac{1}{8}$ & $r_1(P_2)=\frac{1}{24}$ & $r_2(P_2)=0$ & $6$ \\ 
 $r_0(P_3)=\frac{1}{8}$ & $r_1(P_3)=\frac{1}{16}$ & $r_2(P_3)=\frac{1}{48}$ & $4$ \\ 
 $r_0(P_4)=\frac{1}{8}$ & $r_1(P_4)=\frac{5}{48}$ & $r_2(P_4)=\frac{1}{48}$ & $4$ \\ 
 $r_0(P_5)=\frac{1}{8}$ & $r_1(P_5)=\frac{5}{16}$ & $r_2(P_5)=\frac{11}{48}$ & $2$ \\ 
 $r_0(P_6)=\frac{1}{8}$ & $r_1(P_6)=\frac{1}{24}$ & $r_2(P_6)=0$ & $6$ \\ 
 $r_0(P_7)=\frac{1}{8}$ & $r_1(P_7)=\frac{3}{16}$ & $r_2(P_7)=\frac{1}{6}$ & $3$ \\ 
 $r_0(P_8)=\frac{1}{8}$ & $r_1(P_8)=\frac{3}{16}$ & $r_2(P_8)=\frac{1}{3}$ & $1$ \\ \end{tabular}
\label{tble:Summ}
\end{table}
\section{Matrix Representation} \label{sec:matrix}
\subsection{Hyperlink Matrix} \label{sec:hyperlink}
If we represent the PageRank algorithm using matrices, we are able to compute a $ [1 \times \textit{n}]$ row vector, $\boldsymbol{\pi}^T$, the PageRank vector, at each iteration. This vector contains the PageRank values for all web pages in the hyperlink graph. In order to do this we first introduce the concept of the Hyperlink matrix, a $[\textit{n}\times \textit{n}]$ matrix \textbf{H}, which is a row normalized matrix where 
\(\boldsymbol{H_{ij}} = \begin{cases} \frac{1}{\vert P_j \vert} & P_j\in B_{P_i } \\ 0 & \textnormal{otherwise} \end{cases}\). So for Figure \ref{fig:Example} we are able to produce 
\[\textbf{H}=\left(
\begin{array}{cccccccc}
0 & \frac{1}{3} & 0 & \frac{1}{3} & 0 &\frac{1}{3} & 0& 0 \\
0 & 0 &\frac{1}{2}& 0 &\frac{1}{2}& 0 & 0 & 0\\
0 & 0 & 0 & 0 & 1 & 0 & 0 & 0\\
0 & 0 & 0 & 0 & \frac{1}{2} & 0 & \frac{1}{2} & 0\\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 1\\
0 & 0 & 0 & \frac{1}{3} & 0 & 0 & \frac{1}{3} & \frac{1}{3} \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\
0 & 0 & 0 & 0 & \frac{1}{2} & 0 & \frac{1}{2} & 0\\
\end{array}
\right)	\]

The non-zero elements of row \textit{i} relate to the outlinking pages of $P_i$, for example, node 1 has outlinks to pages 2, 4 and 6, and so in columns 2, 4 and 6 of \textbf{H} the value in row 1 are $\frac{1}{3}$ and the non-zero elements of column \textit{i} relates to the inlinking pages.

We are able to write Equation \eqref{eq:iterative} as \begin{equation} \label{eq:power H}
\boldsymbol\pi^{(k+1)T} = \boldsymbol\pi^{(k)T}\textbf{H}
\end{equation}  which shows that the vector $\boldsymbol\pi^T$ is an eigenvector of the matrix \textbf{H} with eigenvalue 1, otherwise known as the stationary vector of \textbf{H}, essentially this is the classical power method applied to \textbf{H}. We are able to observe that \textbf{H} is a very sparse matrix, as the majority of its entries are 0, which is computationally very beneficial, as it reduces the vector-matrix multiplication at each iteration to a linear computation. The formation of \textbf{H} is very similar to the formation of a transition probability matrix for a Markov chain, and it is also noted that \textbf{H} is similar to a stochastic matrix, as all entries are non-negative, however not all rows sum to 1.

The vector $\boldsymbol\pi$ is called a stationary distribution of the chain if $\boldsymbol\pi$ has entries $(\pi_j : j \in S)$ such that:
\begin{itemize}
\item $\pi_j \geq 0$ for all j, and $\sum_j \pi_j = 1$,
\item $\boldsymbol\pi = \boldsymbol\pi\textbf{P}$, so $\pi_j = \sum_i \pi_ip_{ij}$ for all j.
\end{itemize} Using this definition, we know that for any starting vector, we can apply the power method to a matrix, \textbf{P},  which will converge to a stationary distribution as long as \textbf{P} is stochastic, irreducible and aperiodic \cite{grimmett2001probability}. However, \textbf{H} is not guaranteed to converge due to the presence of rank sinks. A rank sink is a page that stores PageRank with each iteration, this can be in the form of dangling nodes, which are nodes that have no outlinks, for example in Figure \ref{fig:dangling},\begin{figure}[h]
\centering
\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=2cm,
                    semithick]
\tikzstyle{every state}=[fill=white,draw=black,text=black]
\node[state] (1) {$1$};
\node[state] (2) [right of=1] {$2$};
\node[state] (3) [right of=2] {$3$};
\path (1) edge node{} (2)
          edge [bend right] node{} (3)
      (3) edge [bend right] node{} (1)
          edge node{} (2);
\end{tikzpicture} \caption{Simple graph with rank sink} \label{fig:dangling} \end{figure} node 2 is a dangling node. There can also be an issue with cycles, for example in Figure \ref{fig:cycle}, 
\begin{figure}[h]
\centering
\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=2cm,
                    semithick]
\tikzstyle{every state}=[fill=white,draw=black,text=black]
\node[state] (1) {$1$};
\node[state] (2) [right of=1] {$2$};
\path (1) edge [bend right] node{} (2)
      (2) edge [bend right] node{} (1);
\end{tikzpicture} \caption{Simple graph with cycle} \label{fig:cycle}
\end{figure} an infinite loop is created between nodes 1 and 2, and so the iterate will not converge. We are able to apply modifications to \textbf{H} which make it a Markov matrix.

\subsection{Stochastic Matrix} \label{sec:stoc}
The first modification that we can apply to \textbf{H}, called the stochasticity adjustment, leads to the formation of a stochastic matrix, \textbf{S}. In the stochasticity adjustment, we change rows of 0's to rows of $\frac{1}{n}$, hence making \textbf{S} stochastic, as all rows now sum to 1. Mathematically this is shown as, 
\begin{equation*} \label{eq:s}
\textbf{S} = \textbf{H} + \textbf{a}\left(\frac{1}{n}\cdot 1^{T}\right)
\end{equation*} 
where \textbf{a} is the dangling node vector, \(\boldsymbol{a} = \begin{cases} 1 & \textnormal{if dangling node} \\ 0 & \textnormal{otherwise} \end{cases}\), and $1^T$ is the rank 1 matrix with unit entries of size $[1\times n]$.

In terms of the random surfer which passes through the matrix, if the surfer enters a dangling node, it is able to teleport to another page in the graph at random. For the 8-node graph example in Figure \ref{fig:Example}, we produce  
\[ \renewcommand*{\arraystretch}{1.25} \textbf{S}=\left(
\begin{array}{cccccccc}
0 & \frac{1}{3} & 0 & \frac{1}{3} & 0 &\frac{1}{3} & 0& 0 \\
0 & 0 &\frac{1}{2}& 0 &\frac{1}{2}& 0 & 0 & 0\\
0 & 0 & 0 & 0 & 1 & 0 & 0 & 0\\
0 & 0 & 0 & 0 & \frac{1}{2} & 0 & \frac{1}{2} & 0\\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 1\\
0 & 0 & 0 & \frac{1}{3} & 0 & 0 & \frac{1}{3} & \frac{1}{3} \\
\frac{1}{8} & \frac{1}{8} & \frac{1}{8} & \frac{1}{8} & \frac{1}{8} & \frac{1}{8} & \frac{1}{8} & \frac{1}{8}\\
0 & 0 & 0 & 0 & \frac{1}{2} & 0 & \frac{1}{2} & 0\\
\end{array}
\right)	\] This modification ensures that \textbf{S} is stochastic, but not that it is irreducible and aperiodic and so it is not guaranteed that it will converge when using the power method.

The stochasticity adjustment is especially crucial when modelling the web in practice, as in an analysis of a significant section of the web graph, only a quarter of the web is strongly connected, and so a large majority of the web are dangling nodes \cite{chakrabarti2002mining}.

\subsection{Google Matrix}\label{sec:google}
The primitivity adjustment is the next modification to the matrix which ensures that it is both aperiodic and irreducible. This modification introduces a scaling parameter $\alpha \in [0,1]$. In terms of the random surfer, this adjustment means that most of the time the surfer will follow links from page $P_i$ to $P_j$, however at any time the surfer could choose a page uniformly at random from the graph and jump there. This adjustment is shown by 
\begin{equation}\label{eq:G}
\textbf{G}=\alpha\textbf{S}+\left((1-\alpha)\cdot\frac{1}{n}\cdot1^T\right)
\end{equation} 
where \textbf{G} is the Google matrix. For Figure \ref{fig:Example},
\[\textbf{G}=\alpha\textbf{S}+\left((1-\alpha)\cdot \frac{1}{8} \cdot \left(  \begin{array}{cccccccc}
1&1&1&1&1&1&1&1\end{array}\right) \right)\]

Due to the primitivity adjustment, \textbf{G} is stochastic, as it is the combination of two stochastic matrices, irreducible and aperiodic. This implies that the power method applied to \textbf{G} is guaranteed to converge to a stationary vector $\boldsymbol{\pi}^T$.
\begin{equation}\label{eq:power to G}
\boldsymbol{\pi}^{(k+1)T} = \boldsymbol{\pi}^{(k)T}\textbf{G}
\end{equation} 

However, \textbf{G} is artificial, as it has been modified twice in order to assure convergence, also \textbf{G} is dense, which is a computational disadvantage, this can be solved by expressing \textbf{G} in terms of \textbf{H}, a very sparse matrix; 
 
\begin{align}
\textbf{G} & = \alpha\textbf{S}+\left((1-\alpha)\cdot\frac{1}{n}\cdot1^T\right)\notag\\
& = \alpha \left(\textbf{H} + \textbf{a}\left(\frac{1}{n}\cdot 1^{T}\right)\right) + \left((1-\alpha)\cdot\frac{1}{n}\cdot1^T\right)\notag\\
& = \alpha\textbf{H} + (\alpha\textbf{a} +(1-\alpha))\left(\frac{1}{n}\cdot 1^T\right)\label{eq:G in H}
\end{align} 

\subsection{$\alpha$ parameter} \label{sec:alpha}
The $\alpha$ parameter introduced in the primitivity adjustment controls how much the original hyperlink structure of the web is weighted. As $\alpha \rightarrow 1$ the expected number of iterations required before convergence increases as the PageRank vector is more volatile, however artificiality is low, and we are close to working with the original hyperlink graph. When $\alpha =0$ then $\textbf{G}=\frac{1}{n}\cdot 1^T$, so there is a link between every page, and so the hyperlink structure has completely disappeared. 

Brin and Page suggested the value of 0.85, as this strikes a balance between efficiency and effectiveness. Using Figure \ref{fig:Example} and Equation \eqref{eq:G in H}, we obtain
\begin{equation*}
\textbf{G} = \left(
\begin{array}{cccccccc}
0.019 & 0.302 & 0.019 & 0.302 & 0.019 & 0.302 & 0.019 & 0.019  \\
0.019 & 0.019 & 0.444 & 0.019 & 0.444 & 0.019 & 0.019 & 0.019  \\
0.019 & 0.019 & 0.019 & 0.019 & 0.869 & 0.019 & 0.019 & 0.019  \\
0.019 & 0.019 & 0.019 & 0.019 & 0.444 & 0.019 & 0.444 & 0.019  \\
0.019 & 0.019 & 0.019 & 0.019 & 0.019 & 0.019 & 0.019 & 0.869  \\
0.019 & 0.019 & 0.019 & 0.302 & 0.019 & 0.019 & 0.302 & 0.302  \\
0.125 & 0.125 & 0.125 & 0.125 & 0.125 & 0.125 & 0.125 & 0.125  \\
0.019 & 0.019 & 0.019 & 0.019 & 0.444 & 0.019 & 0.444 & 0.019 
\end{array}
\right)
\end{equation*}
\section{Solving the PageRank Equation} \label{sec:solve}
In order to solve the PageRank equation we need to solve the eigenvector problem for $\boldsymbol{\pi}^T$, \(\boldsymbol{\pi}^T = \boldsymbol{\pi}^T\textbf{G}\), i.e. we need to find a normalized dominant left-hand eigenvector of \textbf{G} which corresponds to the dominant eigenvalue of $\lambda_1 = 1$. We are also able to solve the equation by using a linear system \(\boldsymbol{\pi}^T(\textbf{I}-\textbf{G})=\textbf{0}^T\), where we want to find a normalized left-hand vector of \textbf{I}-\textbf{G}, \cite{langville}. We will discuss the use of the power method to find the dominant eigenvector in more detail in this report, as this is the original method proposed to solve the PageRank equation, however there are many other methods that we could use which have been detailed in Stewart \cite{stewart1994introduction}.

\subsection{Power Method} \label{sec:power}

When our Markov chain is aperiodic, irreducible and stochastic, the vectors $\boldsymbol\pi^{(k)}$ will converge to the stationary vector $\boldsymbol\pi$, this is not dependant on the initial vector;
\begin{equation*}
\lim_{k\to\infty} \boldsymbol\pi^k = \boldsymbol\pi
\end{equation*}
This iterative method of determining the stationary vector is known as the power method. 

The power method on a matrix, M, is given as the iterative procedure 
\begin{equation} \label{eq:Power iter}
\boldsymbol\pi^{(k+1)} = \frac{1}{\zeta_k}M\boldsymbol\pi^{(k)}
\end{equation} where $\zeta_k$ is a normalizing factor, typically $\zeta_k = \Vert M\pi^{(k)}\Vert_{\infty}$ and $\boldsymbol\pi^{(0)}$ is an arbitrary starting vector. We let M have an eigensolution 
\begin{equation*}
Mx_i = \lambda_ix_i \qquad \textnormal{ where }\qquad  i = 1,2,\ldots,n
\end{equation*} 
where 
\begin{equation*}
\vert\lambda_1\vert>\vert\lambda_2\vert\geq\vert\lambda_3\vert\geq\cdots\geq\vert\lambda_k\vert
\end{equation*}

As we have formed a stochastic Google matrix, \textbf{G}, the dominant eigenvalue is 1, $\lambda_1=1$, we do not need to normalise the iterates. Hence Equation \eqref{eq:Power iter} is now given as
\begin{equation*}
\boldsymbol\pi^{(k+1)} = \textbf{G}^T\boldsymbol\pi^{(k)}
\end{equation*}

As our Google matrix, \textbf{G}, is positive, stochastic and irreducible matrix, by the Perron-Frobenius theorem we are guaranteed to have a positive eigenvector, as shown by statement 3 \cite{thorson2004modeling}.
\begin{theorem*}[Perron-Frobenius Theorem] If \(\textbf{A}_{n \times n} \geq 0\) is irreducible, then each of the following is true.
\begin{enumerate}
\item $r = \rho(\textbf{A}) > 0$.
\item $r \in \sigma (\textbf{A}) $ (\textit{r} is the \textit{Perron root}).
\item $alg mult_\textbf{A}(r) = 1$. (the Perron root is simple).
\item There exists an eigenvector \textbf{x} $>$ \textbf{0} such that \textbf{Ax} = \textit{r}\textbf{x}.
\item The \textit{Perron vector} is the unique vector defined by 
\[\textbf{Ap}=r\textbf{p}, \quad \textbf{p}>\textbf{0}, \quad \Vert \textbf{p} \Vert _1 = 1, \] and, except for positive multiples of \textbf{p}, there are no other nonnegative eigenvectors for \textbf{A}, regardless of the eigenvalue.
\item \textit{r} need not be the only eigenvalue on the spectral circle of \textbf{A}.
\item \(r=\textnormal{max}_{\textbf{x}\in \mathcal{N}} f(\textbf{x})\), (the Collatz-Wielandt formula), 
\begin{equation*}
\textnormal{where } f(\textbf{x}) = \min_{\substack{1\leq i \leq n \\ x_i \neq 0}} \frac{[\textbf{Ax}]_i}{x_i} \textnormal{ and } \mathcal{N} = \{ \textbf{x $\mid \textbf{x} \geq \textbf{0} $ \textnormal{with} \textbf{x} $\neq$ \textbf{0} } \}
\end{equation*}
\end{enumerate}
\end{theorem*} \label{thm:P-F} 

\subsection{Why the Power Method}\label{sec:why power}

The power method is useful for solving the PageRank equation as it converges to a unique vector, the PageRank vector, where the $i^{th}$ entry of $\boldsymbol\pi$ relates to the PageRank of \textit{i} \cite{ipsen2005analysis}. The main disadvantage of the power method is that it known to be very slow to converge, with the computation of the PageRank vector taking a few days to complete for the large web graph, as n is about 54 billion, but the power method is very simple \cite{bonato}. As we are able to simplify \textbf{G} to \textbf{H} as shown in Equation \eqref{eq:G in H}, the power method is very storage friendly, this is as for an iteration only a sparse \textbf{H}, a dangling node vector \textbf{a} and the current iterate are required to be stored \cite{langville}. Brin and Page reported that the PageRank vector converges to 2-3 degrees of accuracy within 50-100 iterations \cite{austin}. Pages with lower PageRank are converge very quickly when compared to pages with higher PageRank \cite{thorson2004modeling}.

\subsection{Worked example}
Using Matlab code as given in Appendix \ref{app:code}, 

\[\boldsymbol\pi = \left(
\begin{array}{c}
0.040 \\
0.051 \\
0.062 \\
0.066 \\
0.258 \\
0.051 \\
0.199 \\
0.274
\end{array}
\right)\]

Hence we are able to produce a ranking of the PageRank vectors as given in Table \ref{tble:PR}. 
\begin{table}[h] \caption{Ranking of web pages after two iterations and after the power method}
 \centering
 \begin{tabular} {c| c} 
 Node & PageRank \\ [0.5ex] 
 \hline
 1&8\\
 2&6\\
 3&5\\
 4&4\\
 5&2\\
 6&6\\
 7&3\\
 8&1\\
 \end{tabular}
 \label{tble:PR}
\end{table}

From this ranking of the nodes in Figure \ref{fig:Example}, we are able to see that node 8 has been given the greatest rank. This is due to the fact that node 5 only points into node 8, and node 5 is pointed to by the majority of the graph, hence node 5 is important, and as it points into node 8 only, node 8 must therefore be the most important node. Equivalently node 1 is the lowest ranked, as it has no inlinks.



%--------------------Comparison to other methods of Ranking web pages
\chapter{Other Methods of Ranking Web Pages}\label{chap:Other}
Whilst PageRank is regarded as the most influential algorithm for ranking web pages, there are many other algorithms that are well known and in some cases preferable. In this chapter we will discuss two such algorithms, Section \ref{sec:HITS} will discuss the HITS algorithm which also ranks web pages by popularity, however differs to PageRank by producing two scores for each web page \cite{kleinberg1999authoritative}. HITS has been used in the search engine Ask.com \cite{bonato}, and expands upon the idea of using connectivity analysis to identify high quality pages within a topic specific graph \cite{manning}. We will discuss the SALSA algorithm in Section \ref{sec:SALSA}, which incorporates ideas from both HITS and PR, as it assigns two scores to each web pages, but uses Markov theory to calculate these scores as in PageRank \cite{lempel2000stochastic}. In Section \ref{sec:compare} we will further explore the strengths and weaknesses of these two algorithms, and compare these algorithms to PageRank.

\section{HITS Algorithm} \label{sec:HITS}
The Hypertext Induced Topic Search (HITS) algorithm was developed by Jon Kleinburg, and is similar to PageRank as it ranks web pages by popularity, however HITS produces two popularity scores, for each web page.  Kleinburg developed this algorithm as method of applying connectivity analysis to the hyper linked environment of the web. The algorithm manages to distil a broad topic down to a representation of a very small size \cite{kleinberg1999authoritative}.

HITS was developed as a way to combat two problems that can occur when ranking web pages. One of these is the scarcity problem, where not many pages contain the exact information that you desire, and it can be difficult to identify the web pages that the user requires. The second problem that can occur is the abundance problem, where the number of pages that are returned to the user as being relevant to the query is too much for the human user to digest. 


The HITS algorithm ranks web pages in terms of their authority and hubness, where an authoritative page is a page with sources of information on a topic, while a hub is a page that is not in itself a source of information, but compilations to authoritative pages \cite{manning}. Authoritative pages have many inlinks and should have relevant content, whilst a hub has many outlinks and point to similar content \cite{baeza1999modern}, in the algorithm it is assumed that each web page on a graph has some measure of an authority and some measure of a hub. A good authority is one which is pointed to by good hubs, whilst a good hub is one which points to good authorities. This cyclic definition is similar to the definition for PR, and we use an iterative computation in order to rank the web pages in the same manner.

The authority weight of a page is the sum of the hubness weights of its parents, which are the backlinks of a node, whilst the hubness weight of a page is the sum of the authority weights of its children, the forward links from a node \cite{baldi2003modeling}.

\subsection{HITS Implementation} \label{sec:HITS implementation}
The HITS algorithm consists of two steps, the first step involves building a neighbourhood graph, N, which is related to the query term, and is known as the sampling step. The second step, the weight-propagation step, involves the computation of the authority and hub scores for each page, two ranked lists are then presented to the user. This computation is an iterative calculation, where firstly each page is assigned an initial authority score $x_i^{(0)}$ and an initial hub score $y_i^{(0)}$, which are then redefined by computing \begin{equation} \label{eq:HITS1}
x_i^{(k)} = \displaystyle \sum_{j:e_{ij}\in E} y_j^{(k-1)} \quad\mathrm{and}\quad y_i^{(k)} = \displaystyle \sum_{j:e_{ij}\in E} x_j^{(k)}  \quad\mathrm{for}\quad k=1,2,3\ldots
\end{equation} 

In the sampling step, the algorithm first creates a root set of pages which contain the query, \textit{q}. Next a base set of pages is formed which includes the root set along with any pages which point into the root set, or which are pointed to, as displayed by Figure \ref{fig:HITS expanding}. 
\begin{figure}[h]
\centering
\includegraphics[width=0.5\linewidth]{expand_baldi.png}
\caption{Expanding the root set into the base set, taken from \cite{baldi2003modeling}}
\label{fig:HITS expanding}
\end{figure}

In practice the base set can become very large and so a maximum number of inlinking and outlinking pages that are added for a particular node in the root set is fixed, for example at 100. The base set is constructed in this manner for several reasons, firstly a good authority page may not contain the query term  as there may be synonyms more commonly used. Also, if the root set contains a good hub set, this expansion means that the base set will then contain good authority pages that it may not have captured previously. The base set then forms the neighbourhood graph, N, which is used in the computation of the authority and hub scores for all pages in N.

Initially the adjacency matrix, \textbf{L}, is formed in the weight-propagation step where \[\textbf{L}_{ij} = \begin{cases} 1, & \textnormal{if there exists an edge from node \textit{i} to node \textit{j}}\\ 0 & \textnormal{otherwise}
\end{cases}\]
If we use the example in Figure \ref{fig:Example} as N for the following calculations, we are able to generate the following adjacency matrix:
\begin{equation*}
\textbf{L}=\left(
\begin{array}{cccccccc}
0 & 1 & 0 & 1 & 0 & 1 & 0 & 0 \\
0 & 0 & 1 & 0 & 1 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 1 & 0 & 1 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 \\
0 & 0 & 0 & 1 & 0 & 0 & 1 & 1 \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 1 & 0 & 1 & 0 \\
\end{array}
\right)
\end{equation*} 

We are able to represent Equations \eqref{eq:HITS1} in terms of the adjacency matrix as \begin{equation} \label{eq:HITSMatrix}
\textbf{x}^{(k)} = \textbf{L}^T\textbf{y}^{(k-1)}\quad\mathrm{and}\quad \textbf{y}^{(k)}=\textbf{Lx}^{(k)}
\end{equation} where \(\textbf{x}^{(k)}\) and \(\textbf{y}^{(k)}\) are $[\textit{n}\times 1]$ vectors which hold the approximate authority and hub scores at each iteration respectively. In the HITS algorithm, we initialise $\textbf{y}^{(0)}$ = \textbf{e}, where \textbf{e} is the column vector of all ones, and then apply Equations \eqref{eq:HITSMatrix} until convergence. 

We are able to further simplify Equations \eqref{eq:HITSMatrix} to \begin{equation} \label{eq:HITSSimplify}
\textbf{x}^{(k)} = \textbf{L}^T\textbf{Lx}^{(k-1)}\quad\mathrm{and}\quad \textbf{y}^{(k)}=\textbf{LL}^T\textbf{y}^{(k-1)}
\end{equation} which shows that we able to use the power method on the matrices $\textbf{L}^T\textbf{L}$ and $\textbf{LL}^T$ respectively. The HITS algorithm defines the matrix $\textbf{L}^T\textbf{L}$ as the Authority matrix, and $\textbf{LL}^T$ as the Hub matrix. So when we apply the power method to these matrices, we compute the authority vector \textbf{x}, which contains the authority scores for all pages in the neighbourhood graph, and the hub vector, \textbf{y}, which similarly contains the hub scores.

Continuing to use Figure \ref{fig:Example} as our example, we produce the following matrices;
\begin{equation*}
%\textnormal{Authority matrix} =
\textbf{L}^T\textbf{L}=\left(
\begin{array}{cccccccc}
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & 1 & 0 & 1 & 0 & 1 & 0 & 0 \\
0 & 0 & 1 & 0 & 1 & 0 & 0 & 0 \\
0 & 1 & 0 & 2 & 0 & 1 & 1 & 1 \\
0 & 0 & 1 & 0 & 4 & 0 & 2 & 0 \\
0 & 1 & 0 & 1 & 0 & 1 & 0 & 0 \\
0 & 0 & 0 & 1 & 2 & 0 & 3 & 1 \\
0 & 0 & 0 & 1 & 0 & 0 & 1 & 2 \\
\end{array}
\right)
\mathrm{,}\quad\mathrm{and}\quad
%\textnormal{Hub matrix} =
\textbf{LL}^T=\left(
\begin{array}{cccccccc}
3 & 0 & 0 & 0 & 0 & 1 & 0 & 0 \\
0 & 2 & 1 & 1 & 0 & 0 & 0 & 1 \\
0 & 1 & 1 & 1 & 0 & 0 & 0 & 1 \\
0 & 1 & 1 & 2 & 0 & 1 & 0 & 2 \\
0 & 0 & 0 & 0 & 1 & 1 & 0 & 0 \\
1 & 0 & 0 & 1 & 1 & 3 & 0 & 1 \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & 1 & 1 & 2 & 0 & 1 & 0 & 2 \\
\end{array}
\right)
\end{equation*} We can note that these matrices are symmetric, positive semi-definite and non-negative, and so HITS with normalization will always converge, with as few as 5 iterations required in order to compute fairly accurate ranking lists \cite{manning}. 

Using the Matlab code given in Appendix \ref{app:code}, we are able to compute authority score \textbf{x}, and hub score, \textbf{y}, vectors
\begin{eqnarray*}
\textbf{x}^T = \left( \begin{array} {cccccccc}
0 & 0.030 & 0.069 & 0.118 & 0.343 &0.030 &0.305 &0.106
\end{array}\right) \\
\textbf{y}^T = \left( \begin{array} {cccccccc}
0.062 & 0.144 & 0.120 & 0.226 & 0.037 & 0.185 & 0 & 0.226
\end{array}\right)
\end{eqnarray*} We note that there is a tie that has occured in the hub rankings for our example, this is unlikely to occur in a larger matrix, however we are able to break this tie using a `first-come, first-served' basis, where if two nodes share the same ranking, as do nodes 4 and 8 in our example, the node that is first in the graph will be given the greater ranking \cite{langville}. Hence we are able to produce two ranked lists for the user in terms of the authority and hub values of the pages in N, as shown in Table \ref{tble:HITS}.

\begin{table}[h] \caption{HITS Authority and Hub rankings}
 \centering
 \begin{tabular} {c| c c} 
 Node & Authority & Hub \\ [0.5ex] 
 \hline
 1&8&6\\
 2&6&4\\
 3&5&5\\
 4&3&1\\
 5&1&7\\
 6&6&3\\
 7&2&8\\
 8&4&2\\
 \end{tabular}
 \label{tble:HITS}
\end{table} 

We are able to note that node 5 is the most authoritative page, this follows from the fact that node 5 has many inlinks. Node 5 is also given a low hub score, as it points only into node 8, which has average authority in the graph. Subsequently, node 8 has a high hub score, as it points into the authoritative node 5. Conversely as node 1 has no outlinks, it is ranked lowest in terms of authority.

\section{SALSA method} \label{sec:SALSA}
In 2000, Ronny Lempel and Schlomo Moran developed an algorithm for ranking web pages called the Stochastic Approach to Link Systems Analysis, SALSA, which incorporates ideas from both HITS and PR. Lempel and Moran developed SALSA in an attempt to avoid certain problems with HITS, such as topic drift, which we will discuss more in Section \ref{sec:compare} \cite{bonato}. The algorithm is similar to HITS as it produces both authority and hub scores for the web pages in the graph, but SALSA examines random walks on two Markov chains derived from the web, similar to PageRank, as opposed to a graph formed from connectivity analysis as in HITS \cite{lempel2000stochastic}. 

\subsection{SALSA Implementation}\label{sec:SALSA implementation}
The SALSA algorithm first forms a neighbourhood graph N in a similar method to HITS, see Section \ref{sec:HITS implementation} for more details, next a bipartite undirected graph, G, is formed. G is defined by three sets, $V_h$, the set of hub nodes, $V_a$, the set of authority nodes, and \textit{E}, which is the set of directed edges in N. Using Figure \ref{fig:Example} as an example, we find 
\[V_h = \{2,3,4,5,6,7,8\} \]
\[V_a = \{1,2,3,4,5,7,8\} \]
The bipartite graph for Figure \ref{fig:Example} is given in Figure \ref{fig:bipartite}, where the authority nodes are on the 'authority side' and similarly the hub nodes are on the 'hub side' of the graph. 

\begin{figure}[h!]
\centering
\begin{tikzpicture}
   \begin{scope}[rotate=90]
       \SetVertexNoLabel
       \grEmptyLadder[RA=2,RB=6]{8}   
       \AssignVertexLabel{a}{8h,7h,6h,5h,4h,3h,2h,1h}
       \AssignVertexLabel{b}{8a,7a,6a,5a,4a,3a,2a,1a}

   \end{scope} 
   \EdgeFromOneToSel{b}{a}{0}{1,3}
   \EdgeFromOneToSel{b}{a}{2}{0,1,4}
   \EdgeFromOneToSel{b}{a}{3}{0}
   \EdgeFromOneToSel{b}{a}{4}{1,3} 
   \EdgeFromOneToSel{b}{a}{5}{3}
   \EdgeFromOneToSel{b}{a}{6}{5,3}
   \EdgeFromOneToSel{b}{a}{7}{2,4,6} 
\end{tikzpicture}\caption{Bipartite graph of Figure \ref{fig:Example} } \label{fig:bipartite}
\end{figure}

The next step in the algorithm relates to the formation of two Markov chains, the authority Markov chain which has a transition probability matrix \textbf{A}, and a hub Markov chain with the matrix \textbf{H}. In relation to the SALSA algorithm \textbf{H} is the hub matrix, which is not to be confused with the hyperlink matrix that \textbf{H} represents in PR, as shown in Section \ref{sec:hyperlink}.  

SALSA uses row and column weighting in order to calculate the authority and hub scores for the neighbourhood graph. Let $\textbf{L}_r$ be the adjacency matrix \textbf{L} with each non-zero row divided by the sum of the rows, and similarly $\textbf{L}_c$ is \textbf{L} where each non zero column is divided by the column sum.

\begin{equation*}
\textbf{L}=\left(
\begin{array}{cccccccc}
0 & 1 & 0 & 1 & 0 & 1 & 0 & 0 \\
0 & 0 & 1 & 0 & 1 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 1 & 0 & 1 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 \\
0 & 0 & 0 & 1 & 0 & 0 & 1 & 1 \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 1 & 0 & 1 & 0 \\
\end{array}
\right)
\end{equation*}

\begin{equation*} \renewcommand*{\arraystretch}{1.25}
\textbf{L}_r=\left(
\begin{array}{cccccccc}
0 & \frac{1}{3} & 0 & \frac{1}{3} & 0 & \frac{1}{3} & 0 & 0 \\
0 & 0 & \frac{1}{2} & 0 & \frac{1}{3} & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 1 & 0 & 0 & 0\\
0 & 0 & 0 & 0 & \frac{1}{2} & 0 & \frac{1}{2} & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 \\
0 & 0 & 0 & \frac{1}{3} & 0 & 0 & \frac{1}{3} & \frac{1}{3} \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & \frac{1}{2} & 0 & \frac{1}{2} & 0 \\
\end{array}
\right)
\mathrm{,}\qquad
\textbf{L}_c=\left(
\begin{array}{cccccccc} 
0 & 1 & 0 & \frac{1}{2} & 0 & 1 & 0 & 0 \\
0 & 0 & 1 & 0 & \frac{1}{4} & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & \frac{1}{4} & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & \frac{1}{4} & 0 & \frac{1}{3} & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & \frac{1}{2} \\
0 & 0 & 0 & \frac{1}{2} & 0 & 0 & \frac{1}{3} & \frac{1}{2} \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & \frac{1}{4} & 0 & \frac{1}{3} & 0 \\
\end{array}
\right)
\end{equation*}

\textbf{H} is formed by removing the zero row and columns in $\textbf{L}_r\textbf{L}_c^T$, and similarly \textbf{A} is formed through removing the zero columns and rows in $\textbf{L}_c^T\textbf{L}_r$.

\begin{equation*} \renewcommand*{\arraystretch}{1.25}
\textbf{L}_r\textbf{L}_c^T=\left(
\begin{array}{cccccc>{\columncolor{yellow}}cc}
\frac{5}{6} & 0 & 0 & 0 & 0 & \frac{1}{6} & 0 & 0 \\
0 & \frac{5}{8} & \frac{1}{8} & \frac{1}{8} & 0 & 0 & 0 & \frac{1}{8} \\
0 & \frac{1}{4} & \frac{1}{4} & \frac{1}{4} & 0 & 0 & 0 & \frac{1}{4} \\
0 & \frac{1}{8} & \frac{1}{8} & \frac{7}{24} & 0 & \frac{1}{6} & 0 & \frac{7}{24} \\
0 & 0 & 0 & 0 & \frac{1}{2} & \frac{1}{2} & 0 & 0\\
\frac{1}{6} & 0 & 0 & \frac{1}{9} & \frac{1}{6} & \frac{4}{9} & 0 & \frac{1}{9} \\
\rowcolor{yellow}
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & \frac{1}{8} & \frac{1}{8} & \frac{7}{24} & 0 & \frac{1}{6} & 0 & \frac{7}{24} \\
\end{array}
\right)
\mathrm{,}\qquad
\textbf{L}_c^T\textbf{L}_r=\left(
\begin{array}{>{\columncolor{yellow}}cccccccc}
\rowcolor{yellow}
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & \frac{1}{3} & 0 & \frac{1}{3} & 0 & \frac{1}{3} & 0 & 0 \\
0 & 0 & \frac{1}{2} & 0 & \frac{1}{2} & 0 & 0 & 0 \\
0 & \frac{1}{6} & 0 & \frac{1}{3} & 0 & \frac{1}{6} & \frac{1}{6} & \frac{1}{6} \\
0 & 0 & \frac{1}{8} & 0 & \frac{5}{8} & 0 & \frac{1}{4} & 0 \\
0 & \frac{1}{3} & 0 & \frac{1}{3} & 0 & \frac{1}{3} & 0 & 0 \\
0 & 0 & 0 & \frac{1}{9} & \frac{1}{3} & 0 & \frac{4}{9} & \frac{1}{9} \\
0 & 0 & 0 & \frac{1}{6} & 0 & 0 & \frac{1}{6} & \frac{2}{3} \\
\end{array}
\right)
\end{equation*}
 Hence we are able to form the SALSA hub and authority matrices for the example;
\begin{equation*} \renewcommand*{\arraystretch}{1.25}
\textbf{H}=\left(
\begin{array}{ccccccc}
\frac{5}{6} & 0 & 0 & 0 & 0 & \frac{1}{6} & 0 \\
0 & \frac{5}{8} & \frac{1}{8} & \frac{1}{8} & 0 & 0 & \frac{1}{8} \\
0 & \frac{1}{4} & \frac{1}{4} & \frac{1}{4} & 0 & 0 & \frac{1}{4} \\
0 & \frac{1}{8} & \frac{1}{8} & \frac{7}{24} & 0 & \frac{1}{6} & \frac{7}{24} \\
0 & 0 & 0 & 0 & \frac{1}{2} & \frac{1}{2} & 0\\
\frac{1}{6} & 0 & 0 & \frac{1}{9} & \frac{1}{6} & \frac{4}{9} & \frac{1}{9} \\
0 & \frac{1}{8} & \frac{1}{8} & \frac{7}{24} & 0 & \frac{1}{6} & \frac{7}{24} \\
\end{array}
\right)
\mathrm{,}\quad\mathrm{and}\quad
\textbf{A}=\left(
\begin{array}{ccccccc}
\frac{1}{3} & 0 & \frac{1}{3} & 0 & \frac{1}{3} & 0 & 0 \\
0 & \frac{1}{2} & 0 & \frac{1}{2} & 0 & 0 & 0 \\
\frac{1}{6} & 0 & \frac{1}{3} & 0 & \frac{1}{6} & \frac{1}{6} & \frac{1}{6} \\
0 & \frac{1}{8} & 0 & \frac{5}{8} & 0 & \frac{1}{4} & 0 \\
\frac{1}{3} & 0 & \frac{1}{3} & 0 & \frac{1}{3} & 0 & 0 \\
0 & 0 & \frac{1}{9} & \frac{1}{3} & 0 & \frac{4}{9} & \frac{1}{9} \\
0 & 0 & \frac{1}{6} & 0 & 0 & \frac{1}{6} & \frac{2}{3} \\
\end{array}
\right)
\end{equation*}

Both the authority and the hub matrix are primitive, as the Markov chains for both are aperiodic \cite{lempel2000stochastic}. If G is connected, then \textbf{H} and \textbf{A} are both irreducible Markov chains, and we are able to find the stationary vector, $\boldsymbol\pi_h^T$, of \textbf{H} which gives the hub scores for N. Equivalently $\boldsymbol\pi_a^T$, the stationary vector of \textbf{A} gives the authority scores. By the ergodic theorem \cite{gallager1992discrete}, we can find the stationary distribution of the underlying Markov chain, as this is the principal eigenvector of an irreducible, aperiodic, stochastic matrix. The entries with the highest entries relate the the nodes that are most frequently visited by the (infinite) random walk. 

In our example, G in Figure \ref{fig:bipartite} is not connected, as we are not able to reach nodes 7a and 1h though any of the other nodes. Due to this, our \textbf{H} and \textbf{A} matrices contain multiple connected graphs, and so in order to calculate the global hub and authority scores for the graph, we must first find the stationary vectors for each connected graph. \textbf{H} contains two connected sets, \textit{X} = \{7\}, and \textit{Y}=\{1,2,3,4,5,6,8\}, \textbf{A} also contains two sets, \textit{W}=\{1\} and \textit{Z}=\{2,3,4,5,6,7,8\}, we are able to compute the stationary values for these connected sets through the use of the power method, as discussed in Section \ref{sec:solve}.

The stationary vectors, $\boldsymbol\pi_h^T$, for the two Markov chains which make up the Hub Markov chain, are 
\[\boldsymbol\pi_h^T (X) = \begin{blockarray}{c}
7 \\
\begin{block}{(c)}
 1 \\
\end{block}
\end{blockarray} \mathrm{,}\quad\mathrm{and}\quad
\boldsymbol\pi_h^T (Y) =
\begin{blockarray}{ccccccc}
1 & 2 & 3 & 4 & 5 & 6 & 8 \\
\begin{block}{(ccccccc)}
 0.214 & 0.143 & 0.071 & 0.143 & 0.071 & 0.214 & 0.143 \\
\end{block}
\end{blockarray} \] correspondingly the stationary values for the two components in \textbf{A} are
\[\boldsymbol\pi_a^T (W) = \begin{blockarray}{c}
1 \\
\begin{block}{(c)}
 1 \\
\end{block}
\end{blockarray} \mathrm{,}\quad\mathrm{and}\quad
\boldsymbol\pi_a^T (Z) = \begin{blockarray}{ccccccc}
2 & 3 & 4 & 5 & 6 & 7 & 8 \\
\begin{block}{(ccccccc)}
0.071 & 0.071 & 0.143 & 0.286 & 0.071 & 0.214 & 0.143 \\
\end{block}
\end{blockarray}\]
From these components we are able to calculate the local Hub and Authority rankings:
\[\boldsymbol\pi_h^T (X) = \begin{blockarray}{c}
7 \\
\begin{block}{(c)}
 1 \\
\end{block}
\end{blockarray} \mathrm{,}\quad\mathrm{and}\quad
\boldsymbol\pi_h^T (Y) =
\begin{blockarray}{ccccccc}
1 & 2 & 3 & 4 & 5 & 6 & 8 \\
\begin{block}{(ccccccc)}
1 & 3 & 6 & 3 & 6 & 1 & 3 \\
\end{block}
\end{blockarray} \]
\[\boldsymbol\pi_h^T (W) = \begin{blockarray}{c}
1 \\
\begin{block}{(c)}
 1 \\
\end{block}
\end{blockarray} \mathrm{,}\quad\mathrm{and}\quad
\boldsymbol\pi_h^T (Z) = \begin{blockarray}{ccccccc}
2 & 3 & 4 & 5 & 6 & 7 & 8 \\
\begin{block}{(ccccccc)}
5 & 5 & 3 & 1 & 5 & 2 & 3  \\
\end{block}
\end{blockarray} \]

We are now able to calculate the global Hub and Authority scores, this is done by weighting the local scores by the proportion of G that they contain, for example \textit{X} contains $\frac{1}{8}$ of the nodes in the hub set, and so SALSA proposes that the $\boldsymbol\pi_h^T(X)$ vector should be weighted by $\frac{1}{8}$. This leads to the global scores;
\begin{eqnarray*}
\pi_h = \left( \begin{array} {cccccccc}
0.188 & 0.125 & 0.063 & 0.125 & 0.063 & 0.188 & 0.125 & 0.125
\end{array}\right) \\
\pi_a = \left( \begin{array} {cccccccc}
0.125 & 0.063 & 0.063 & 0.125 & 0.250 & 0.063 & 0.188 & 0.125
\end{array}\right)
\end{eqnarray*} we are able to break the ties in the rankings by using a 'first-come, first-served' basis as we have done prior in Section \ref{sec:HITS implementation} , and so we produce the rankings as shown in Table \ref{tble:SALSA}

\begin{table}[H] \caption{SALSA Authority and Hub rankings}
 \centering
 \begin{tabular} {c| c c} 
 Node & Authority & Hub \\ [0.5ex] 
 \hline
 1&3&1\\
 2&6&3\\
 3&7&7\\
 4&4&4\\
 5&1&8\\
 6&8&2\\
 7&2&5\\
 8&5&6\\
 \end{tabular}
 \label{tble:SALSA}
\end{table}

\section{Comparing PageRank, HITS and SALSA} \label{sec:compare}
\begin{table}[H] \caption{Page rankings for Figure \ref{fig:Example} using PR, HITS and SALSA }
 \centering
 \begin{tabular} {c| c| c| c| c| c} 
 \multirow{2}{*}{Node} & \multirow{2}{*}{PR} & \multicolumn{2}{|c|}{HITS} & \multicolumn{2}{|c}{SALSA} \\ [0.5ex] 
 {}&{}&Authority & Hub & Authority & Hub\\ 
 \hline
 1&8&8&6&3&1\\
 2&6&6&4&6&3\\
 3&5&5&5&7&7\\
 4&4&3&1&4&4\\
 5&2&1&7&1&8\\
 6&6&6&3&8&2\\
 7&3&2&8&2&5\\
 8&1&4&2&5&6\\
 \end{tabular}
 \label{tble: comparison}
\end{table}
\subsection*{Accuracy}

Spamming can negatively impact the accuracy of the ranking produced by an algorithm, one of the methods of spam that can occur is spamdexing. Spamdexing is an intentional attempt to improve the ranking of a page on a search engine, this can be done by link spam or by link farms. Link farms are densely connected sub graphs of the web which have been artificially constructed in order for a certain set of web pages to accumulate ranking, this is shown in Figure \ref{fig:link farm},
\begin{figure}[h]
\centering
\includegraphics[width=0.5\linewidth]{link_farm_baldi.png}
\caption{A link farm, taken from \cite{baldi2003modeling}}
\label{fig:link farm}
\end{figure}
where the shaded nodes are copies of the same page and as such one page will artificially rank highly \cite{baldi2003modeling}. 


PageRank is sensitive to in-degree, and so a simple way to spamdex PageRank is to increase the in-degree to your page using methods such as link farms to add more inlinks to your page \cite{bonato}. HITS and SALSA are more sensitive to spamdexing than PageRank due to the mutual reinforcement effect, where by adding and removing links to and from a web page you are able to influence the authority and hub scores for the whole graph as they share a mutual dependence, as shown in Equations \eqref{eq:HITSSimplify}. SALSA is less sensitive to spamming than HITS as the coupling between the hub and authority scores is less strict \cite{lempel2000stochastic}.

Another factor which may impact the accuracy of the rankings provided for the user is topic drift, which is where the highest ranked pages for a certain topic do not necessarily pertain to that topic. This commonly this occurs during the construction of the neighbourhood graph in the HITS and SALSA algorithms, as it is possible that during the expansion of the root set, an authoritative yet off-topic page can be linked, which has a knock-on effect of skewing the ranking to off-topic pages. The general solution to this problem as suggested by Bharat and Herzinger is to use content analysis \cite{bharat1998improved}. Using this improved algorithm, precision over the original HITS algorithm has been shown to increase by 45\%, however this is limited to topics that are already well-represented and connected on the web.

\subsection*{TKC Effect}

HITS relies on using connectivity analysis on the web graph, however there are three main problems which can arise through this, such as mutually reinforced relationships in a tightly knitted community, TKC, where a set of documents on page A could point to a single document on page B which would then drive up the hub score of page A, whilst increasing the authority ranking on page B. When this occurs in a TKC, pages that may have no authority on a topic or may pertain to just one aspect of the query may be ranked highly \cite{lempel2000stochastic}. SALSA is less vulnerable to the TKC effect, and has been shown to be the more accurate ranking algorithm in instances where the TKC effect occurs \cite{lempel2000stochastic}.

There is also a problem of automatically generated links, which would not represent human opinion, also topic-drift arises through the usage of connectivity analysis, where when building N, it is possible for an authoritative page which is off-topic can be linked to pages which contain the query term, and so can skew the rankings to off-topic pages.  

HITS emphasises the mutual reinforcement between authority and hub web pages, whereas PageRank focuses more on web surfing based on random walk models \cite{ding2003pagerank}.


\subsection*{Dual Rankings}
An advantage of HITS and SALSA compared to PageRank is the dual ranking that the algorithms assign to each web page. A user may find it useful to know which pages are the most authoritative on a given query, or it may be more useful for to know which pages are good hubs of knowledge.

\subsection*{Query-dependence}
HITS and SALSA are query-dependant ranking algorithms, this is as they first generate a root set which is dependant on the query, and then return two real numbers for a given page that the algorithms deem relevant to the topic. PageRank however is query-independent as all pages in the web graph are ranked by importance at crawl time with no regard for the query.This query-dependence is a weakness of HITS and SALSA, as for each query new rankings must be computed, however the ranking does take into account the query that the user is requiring more knowledge on unlike PageRank. It is possible to make HITS and SALSA query dependant by removing the sampling step of the algorithm, hence reducing computation time. 

\subsection*{Uniqueness}
A problem can arise with the uniqueness of the vectors produced in the HITS and SALSA algorithms, as different authority and hub vectors are computed by different choices of the initial vector \cite{langville}. The output of HITS also depends on whether the implementation uses the initial authority or hub vector as it is not required to apply the power method on both the authority matrix and the hub matrix. This is as we are able to use Equations \eqref{eq:HITSMatrix} in order to calculate one of the vectors from the other. PageRank doesn't display this inconsistency in the PageRank vector \cite{farahat2006authority}. 

A ranking algorithm that assigns both hub and authority weights is badly behaved on G if the final results is dependant on on initial vectors, or if a node is given authority or hub weight of zero, even if it has either an in-degree or out-degree which is greater than zero \cite{bonato}.

HITS converges to a non-unique solution as the authority matrix is not irreducible, we are able to modify HITS in a manner similar to the primitivity adjustment in PR, as discussed in section \ref{sec:matrix}. In this modification, we can create a modified authority matrix 
\[\xi\textbf{L}^T\textbf{L} +\frac{(1-\xi)}{n}\textbf{ee}^T\] 
can be created, where $0<\xi<1$ \cite{ng2001stable}, and similarly create a hub matrix,
\[\xi\textbf{LL}^T +\frac{(1-\xi)}{n}\textbf{ee}^T\] 
These modified matrices are now irreducible, and by the Perron-Frobenius theorem, as discussed in Section \ref{sec:power}, we can find a unique, normalised, positive dominant eigenvector \cite{meyer2000matrix}.

\subsection*{Computation costs and Sensitivity}
A disadvantage of the PageRank algorithm is the relative large computation costs, this is due to the algorithm having to be performed on the web graph as a whole, also the structure and content of the web changes daily, leading to the `Google Dance' which must be done monthly \cite{thorson2004modeling}. It has been shown that in a sample of web pages, 40\% of the web pages change their content weekly, whilst 23\% of the .com pages in the set changed daily \cite{cho1999evolution}. 

HITS is insensitive to minor changes in its neighbourhood graph, N, PageRank can be sensitive depending on where the change occurred. If the change in the web graph occurred with pages that have a low PageRank value, then the overall PageRank is very small \cite{ng2001stable}.

HITS is usually described as running on a smaller collection of articles that have been retrieved on a query topic, whilst PageRank runs on the entire web \cite{ng2001link}. As we form \textbf{L} through the base set, the order of \textbf{L} is much smaller then the total number of pages on the web, and so the computation required in producing a ranked list of the web pages is less costly than the computation in PR, we also do not need to compute eigenvectors for both matrices, as we are able to use the equations, which is computationally simpler. Figure \ref{fig:N build}
\begin{figure}[h] 
\centering
\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=2cm,
semithick]
\tikzstyle{every state}=[fill=white,draw=black,text=black]
\node[state] (1) {$1$};
\node[state] (2) [right of=1] {$2$};
\node[state] (3) [right of=2] {$3$};
\node[state] (4) [below of=2] {$4$};
\node[state] (5) [below of=3] {$5$};
\node[state] (6) [below of=4] {$6$};
\path (1) edge node{} (2)
          edge [bend left] node{} (3)
          edge node{} (4)
          edge node{} (6)
      (2) edge node{} (4)
      (3) edge node{} (2)
          edge node{} (4)
          edge node{} (5)
      (4) edge node{} (5)
      (6) edge node{} (5)
          edge node{} (4);
\end{tikzpicture} \qquad\qquad
\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=2cm,
semithick]
\tikzstyle{every state}=[fill=white,draw=black,text=black]
\node[state] (1) {$1$};
\node[state] (2) [ultra thick, right of=1] {$2$};
\node[state] (3) [right of=2] {$3$};
\node[state] (4) [below of=2] {$4$};
\node[state] (5) [ultra thick, below of=3] {$5$};
\node[state] (6) [below of=4] {$6$};
\path (1) edge node{} (2)
      (2) edge node{} (4)
      (3) edge node{} (2)
          edge node{} (5)
      (4) edge node{} (5)
      (6) edge node{} (5);
\end{tikzpicture}
\caption{Neighbourhood graph, N,  for nodes \{2,5\} } \label{fig:N build}
\end{figure}
 shows how the N graph that is built through the HITS and SALSA algorithms is smaller than the web graph, and so it is less computationally taxing on a system.

SALSA is less computationally taxing than HITS \cite{lempel2000stochastic}, this is due to the multiply connected graphs present, and so the power method only needs to be applied to smaller Markov chains. SALSA also does not force connectedness across the web graph as PR does, and therefore is less artificial. 

%--------------------Improvements
\chapter{Improvements to the PageRank Algorithm} \label{chap:Improve}
In this chapter we will discuss various approaches to improve the PageRank algorithm in order to increase the accuracy of the PageRank vector for a user. The original algorithm is simplistic in the assumption that a random surfer will move throughout the hyperlink graph with equal probability of visiting any page in the web graph, and so it is not a very good model of human behaviour, as a surfer will be more drawn to pages that interest them specifically. We will discuss various methods which have been developed in order to make the PageRank algorithm more realistic, such as the intelligent surfer in Section \ref{sec:intelligent}, or the use of a personalisation vector in Section \ref{sec:Personalization}. We will explore this personalised PageRank more with respect to the development of a Topic-specific PageRank algorithm in Section \ref{sec:TSPR}. Another modified algorithm that we will focus on is the Weighted PageRank which takes into account the importance of the outlinks and inlinks a page has in Section \ref{sec:weighted}. 

\section{Intelligent Surfer} \label{sec:intelligent}

The PageRank algorithm ranks a page highly if it is at the centre of a hyper-graph, however a human surfer would prefer the highest ranked pages to also be relevant to the query. Richardson and Domingos proposed the implementation of an intelligent surfer to the PageRank algorithm as a solution to this problem in their development of a query-dependent PageRank algorithm, QDPR \cite{richardson2002intelligent}. The intelligent surfer will be more likely to jump to a page which contains more content on the query, and so these pages should be assigned a greater weight \cite{langville}.

In the QDPR algorithm, instead of a normal probability distribution, the probability distribution,$P_q(j)$, over the pages in the web graph is 
\begin{equation}\label{eq:QDPR}
P_q(j)=(1-\alpha)P_q'(j)+\alpha\sum_{i\in B_j}P_q(i)P_q(i\rightarrow j)
\end{equation}
where $P_q(i\rightarrow j)$ is the probability that the surfer chooses to jump to page \textit{j} from page \textit{i} while searching for query \textit{q}, $P_q'(j)$ is the probability that the surfer teleports to a page at random in the web graph. These probability distributions in Equation \eqref{eq:QDPR} can be arbitrarily produced from using access logs in order to find the tendencies of the surfer \cite{langville}, we will explore the instance when the distributions have been derived from a measure of the relevancy of the web page to the query in question, $R_q(j)$.

\begin{equation}\label{eq:relevant}
P_q'(j) = \frac{R_q(j)}{\sum_{k\in W} R_q(k)} \quad P_q(i\rightarrow j) = \frac{R_q(j)}{\sum_{k\in F_i}R_q(k)}
\end{equation}
where W is the set of all web pages, and $F_i$ is the set of pages which \textit{i} links into. Essentially Equations \eqref{eq:QDPR} and \eqref{eq:relevant} ensure that when a surfer is choosing which out-link to follow, they will be more likely to follows links leading to pages which have been deemed relevant to their query.

The algorithm also allows for a user to search for a multi-term query, $Q=\{q_1,q_2 \cdots \}$, where the resulting probability distribution is \begin{equation} \label{eq:multi QDPR}
P_Q(j) = \sum_{q\in Q}P(q)P_Q(j)
\end{equation}

The use of an intelligent surfer outperforms the standard PageRank algorithm in terms of the quality of the pages, whilst still being efficient enough in terms of being a practical search engine. The efficiency is ensured by pre-computing the individual term rankings during the offline crawl, and then these are combined at query time using Equation \eqref{eq:multi QDPR}. Implementing an intelligent surfer can be viewed as a solution to topic drift, as discussed in Section \ref{sec:compare}, as relevant pages to the given topic will have greater weight. Richardson and Domingos found that their QDPR algorithm also maintains advantages of the standard PR, namely the minimal storage required for implementation, as to carry out the algorithm on hundreds of thousands of query terms, only 100-200 times the storage of the PR algorithm is required \cite{richardson2002intelligent}.

\section{Personalization Vector} \label{sec:Personalization}

We are able to form a personalised PageRank which `can create a view of Web from a particular perspective' which will be specific to the user \cite{page1999pagerank}. We can do this by using $\textbf{ev}^T$, an arbitrary stochastic vector in the general PageRank algorithm instead of $\frac{1}{n}\cdot1^T$ \cite{bonato}. Hence the Google matrix in Equation \eqref{eq:G} is now given as 
\begin{equation}\label{eq:Personalisation G}
\textbf{G}=\alpha\textbf{S}+\left((1-\alpha)\cdot\textbf{ev}^T\right)
\end{equation} 
We are now able to state the PageRank equation Equation \eqref{eq:power to G} as; 
\begin{align}
\boldsymbol{\pi}^{(k+1)T} &= \boldsymbol{\pi}^{(k)T}\textbf{G} \nonumber \\
&= \alpha\boldsymbol{\pi}^{(k)T}\textbf{S} + (1-\alpha)\boldsymbol{\pi}^{(k)T}\textbf{ev}^T \nonumber \\
&= \alpha\boldsymbol{\pi}^{(k)T}\textbf{H} + \left(\alpha\boldsymbol{\pi}^{(k)T}\textbf{a} + 1 - \alpha\right)\textbf{v}^T \label{eq:Power personalization}
\end{align}
When we use this personalization vector, the probabilities are no longer uniformly distributed \cite{langville}. The use of the personalization vector retains the advantage of being able to use the power method in order to compute the rankings, along with maintaining the other advantages of PageRank such as needing minimal storage required in order to generate the rankings.

Personalized PageRank changes only the PageRank vector itself, as different personalizations produce different rankings \cite{thorson2004modeling}. The personalization vector can also be used as an attempt to control spamming by link farms, as PageRank is vulnerable to this spamdexing as discussed in Section \ref{sec:compare}. However personalized PageRank is costly in implementation as it is very difficult to form the personalization vector, and so in order to combat this we use pseudo-personalization PageRank systems which are more viable in practice. These systems aim to apply personalisations for specific groups of users instead of customising the algorithm for each user. The most well-known of these pseudo-personalization systems is Haveliwala's Topic-Specific PageRank which we will further explore in the next section.

\section{Topic-Sensitive PageRank} \label{sec:TSPR}

Haveliwala suggested a pseudo-personalised PageRank system, the Topic-Sensitive PageRank algorithm, TSPR, which aims to group users in order to lessen the computation of unique personalised PageRank for each user \cite{haveliwala2002topic}. TSPR suggests that the web page can be represented sufficiently by 16 main topics generated from the Open Directory Project \cite{ODP};
\begin{multicols}{4}
\begin{itemize}
\item Arts
\item Business
\item Computers
\item Games
\item Health
\item Home
\item Kids \& teens
\item News
\item Recreation
\item Reference
\item Regional
\item Science
\item Shopping
\item Society
\item Sports
\item World
\end{itemize} 
\end{multicols}
During the initial offline crawl 16 topic-sensitive PageRank vectors are generated which have each been biased to one of the above topics \cite{haveliwala2002topic, langville}. This information can be created automatically from an analysis of the bookmarks and browser history over time, or the user could manually input their preference for each topic. The algorithm assumes that an individuals interest can be approximated sufficiently as a linear combination of the smaller topic page distributions \cite{manning}.

At query time, the algorithm calculates the similarity of the query to each of these topics, and then takes a linear combination of the TSPR vectors which have been weighted with respect to the similarity of the users query to the topic. The Topic-Sensitive PageRank vectors are denoted as $\boldsymbol{\pi}_t$ where \textit{t} denotes the topic, for example the sports PageRank vector would be $\boldsymbol{\pi}_s$. We can assume that each user will have their own unique mixture of interests, for example User P could be interested in sports 60\%, but also interested in politics 40\%, and so the TSPR vector would be $\boldsymbol{\pi}_P=0.6\pi_s+0.4\pi_p$, leading to a Markov chain with a steady state distribution that has been personalized.  

We are now able to express Equation \eqref{eq:Personalisation G} as 
\begin{equation*}\label{eq:TSPR G}
\textbf{G}=\alpha\textbf{S}+\left((1-\alpha)\cdot\textbf{e}\boldsymbol{\pi}_t^T\right)
\end{equation*} 
and Equation \eqref{eq:Power personalization} becomes ; We are now able to state the PageRank equation Equation \eqref{eq:power to G} as; 
\begin{equation*}
\boldsymbol{\pi}^{(k+1)T} = \alpha\boldsymbol{\pi}^{(k)T}\textbf{H} + \left(\alpha\boldsymbol{\pi}^{(k)T}\textbf{a} + 1 - \alpha\right)\boldsymbol{\pi}_t^T \label{eq:TSPR power}
\end{equation*} This can be shown in Figure \ref{fig:topic-specific}, when $\alpha$ is 0.9, the user P will teleport 6\% to sports pages, and 4\% to politics pages.

\begin{figure}[h]
\centering
\includegraphics[width=0.75\linewidth]{Topic-specific_PageRank_Manning.png}
\caption{Topic-Sensitive PageRank, where a user has 60\% interest in sports, and 40\% interest in politics, taken from \cite{manning}}
\label{fig:topic-specific}\end{figure}

The Topic-Sensitive PageRank vector generates more accurate ranking than the single generic PageRank vector, as a certain user will be more interested in some topics, and so intuitively these groups of pages should have a higher ranking. The implementation of the algorithm is costly as for each user there is a need to compute a unique transition matrix \cite{manning}. TSPR is a query-dependant algorithm, which allows the query to impact the ranking, however like PageRank there is minimal query-time processing. As the computation of the Topic-Sensitive PageRank is done prior to query-time, the algorithm doesn't have large query-time costs \cite{haveliwala2002topic}. TSPR is also less vulnerable to topic drift, as we avoid the problem of heavily linked web pages having a high ranking for queries that they have little authority. 

\section{Weighted PageRank}\label{sec:weighted}

Weighted PageRank algorithm differs from the standard algorithm by assigning more value to important pages rather than diving the rank equally, as each outlink page receives a value which is proportional to its popularity \cite{xing2004weighted}. $W^{in}_{(j,i)}$ is the weight of the link between pages \textit{i} and \textit{j}, which is calculated as 
\begin{equation}\label{eq:weightin}
W^{in}_{(j,i)} = \frac{I_i}{\sum_{k\in R(i)}I_k}
\end{equation}
where $I_i$ and $I_k$ represent the number of inlinks of pages \textit{i} and \textit{j} respectively, and $R(j)$ is the set of pages that \textit{j} links into. 
Similarly $W^{out}_{(j,i)}$ is calculated as
\begin{equation}\label{eq:weightout}
W^{out}_{(j,i)} = \frac{O_i}{\sum_{k\in R(i)}O_k}
\end{equation}

In the example Figure \ref{fig:Example}, Page 2 has 2 outlinks, 3 and 5. The inlinks and outlinks of these pages are, $I_3 = 1$, $I_5 = 4$, $O_3 = 1$ and $O_5 = 1$, and so we can calculate 
\[W^{in}_{(2,3)} = \frac{I_3}{I_3 + I_5} = \frac{1}{5} \]
and 
\[W^{out}_{(2,3)} = \frac{O_3}{O_3 + O_5} = \frac{1}{2} \] 
Applying Equations \eqref{eq:weightin}, \eqref{eq:weightout} to Figure \ref{fig:Example} we are able to produce the following matrices
\[ \textbf{W}_{in}=\left(
\begin{array}{cccccccc}
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\
0 & 1 & 0 & 0 & 0 & 0 & 0 & 0\\
0 & 0 & 0 & 0 & 0 & 1 & 0 & 0\\
0 & \frac{1}{6} & \frac{1}{6} & \frac{1}{3} & 0 & 0 & 0 & \frac{1}{3}\\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & \frac{2}{5} & 0 & \frac{1}{5} & 0 & \frac{2}{5}\\
0 & 0 & 0 & 0 & \frac{4}{5} & \frac{1}{5} & 0 & 0\\
\end{array}
\right)\mathrm{,}\quad\mathrm{and}\quad \textbf{W}_{out}=\left(
\begin{array}{cccccccc}
0 & \frac{2}{7} & 0 & \frac{2}{7} & 0 &\frac{3}{7} & 0 & 0 \\
0 & 0 &\frac{1}{2}& 0 &\frac{1}{2}& 0 & 0 & 0\\
0 & 0 & 0 & 0 & 1 & 0 & 0 & 0\\
0 & 0 & 0 & 0 & 1 & 0 & 0 & 0\\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 1\\
0 & 0 & 0 & \frac{1}{2} & 0 & 0 & 0 & \frac{1}{2} \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\
0 & 0 & 0 & 0 & 1 & 0 & 0 & 0\\
\end{array}
\right)\]
The original PageRank equation given in Equation \eqref{eq:PR}, is now modified to 
\begin{equation} \label{eq:WPR}
r(P_i) = (1-\alpha) + \alpha \displaystyle \sum_{P_j\in B_{P_i }} r(P_j) \cdot W^{in}_{(j,i)} \cdot W^{out}_{(j,i)}
\end{equation}
Representing Equation \eqref{eq:WPR} in terms of a matrix, we are able to produce a hyperlink matrix which is different to \textbf{H} produced in section \ref{sec:hyperlink}

\[\textbf{H}_{WPR}=\left(
\begin{array}{cccccccc}
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\
0 & 0 & \frac{1}{2} & 0 & \frac{1}{2} & 0 & 0 & 0\\
0 & 0 & 0 & \frac{1}{2} & 0 & 0 & 0 & \frac{1}{2}\\
0 & 0 & \frac{1}{12} & 0 & \frac{11}{12} & 0 & 0 & 0\\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & \frac{1}{10} & \frac{8}{10} & 0 & 0 & \frac{1}{10}\\
0 & 0 & 0 & \frac{1}{10} & 0 & 0 & 0 & \frac{9}{10}\\
\end{array}
\right)	\]

We are now able to solve for the weighted PageRank vector, $\boldsymbol\pi_{WPR}$, through the power method as discussed in chapters 2,
\[\boldsymbol\pi_{WPR} = \left(
\begin{array}{c}
0.028 \\
0.028 \\
0.097 \\
0.097 \\
0.395 \\
0.028 \\
0.028 \\
0.302
\end{array}
\right)\]

We can note that from our example there are a large proportion of tied PageRank vectors, this can be solved simply on a `first-come, first-served' basis \cite{langville}. Hence we are then able to compare the PageRankings produced by performing the standard algorithm and the weighted algorithm, as in Table \ref{tble:Weighted PageRank and PR}. 

\begin{table}[h] \caption{Comparison of PageRankings produced by standard algorithm and the weighted PageRank algorithm}
 \centering
 \begin{tabular} {c|c c} 
 Node & PageRank & Weighted PageRank \\ [0.5ex] 
 \hline
 1&8&5\\
 2&6&6\\
 3&5&3\\
 4&4&4\\
 5&2&1\\
 6&6&7\\
 7&3&8\\
 8&1&2\\
 \end{tabular}
 \label{tble:Weighted PageRank and PR}
\end{table}

We are able to note that there are some differences in the rankings produced by the PageRank algorithm and the WPR algorithm, notably that node 7 was ranked 3rd in the standard PR, and 8th when we use WPR. We can justify this result by looking at the lack of outlinks from node 7, as WPR takes into account both inlinks and outlinks from a node when calculating its ranked list, the fact that node 7 doesn't link into any other node in the network means that it is ranked low in terms of importance, even though it has three nodes which point into it, which is why node 7 has been ranked highly by the standard algorithm. We can apply this same logic to understand why node 5 is now the most highly ranked node rather than node 8, which intuitively fits, as node 5 has the greatest number of links to the rest of the network. 

The Weighted PageRank algorithm has been shown as being able to identify a greater number of relevant pages when compared to the standard PageRank algorithm \cite{xing2004weighted}. We will use the Weighted PageRank algorithm later in this report in section \ref{sec:Roads}, in order to predict the human movement in an urban space.

The WPR algorithm has been expanded on by many other ranking algorithms, for example Jaganathan and Desikan developed the Weighted PageRank algorithm which is based on the In-Out weights of each link in 2015, which has been shown to be comparable to the standard PR algorithm with respect to computation time \cite{jaganathan2015weighted}. 

Another algorithm is the Weighted Links Rank, WL Rank, which considers different web page attributes to give different weights to some links \cite{baeza2004web}. This algorithm gives more weight to links at the beginning of pages, and has been found to be more accurate than the PageRank algorithm in terms of the precision of the results presented to the user.

%--------------------Applications
\chapter{Applications of the PageRank algorithm} \label{chap:Applications}
The PageRank algorithm can also be used in many varied applications beyond ranking web pages, which we explore in this chapter. We will explore the algorithm with respect to modelling traffic in Durham in more detail, alongside applications in chemistry and in recommender systems. 

The PageRank algorithm can be used as a measure of network centrality, where you are able to understand a graph better when PageRank reveals what is most important. We are also able to apply modified PageRank to some applications, for example reverse PageRank can be used to model food chains, where a species,\textit{i}, is important if it feeds a species, \textit{j} \cite{allesina2009googling}. Some other applications are in sports network, with the idea that a fan of a team will follow them until they are beaten, at which time they will support the new team, in biology as an attempt to understand the impact of certain genes \cite{gleich2015pagerank}. Winter et al. have shown that the PageRank alogrithm has been useful in determining genes which strongly predict survival in patients with pancreatic ductal adenocarcinoma, a type of cancer with a low prognosis \cite{winter2012google}.

\section{Wikipedia} \label{sec:wiki}
We are able to apply PageRank to large databases in order to gain insight into certain topics, for example we are able to generate reading lists for university courses automatically from exploiting the link structure of Wikipedia, as we abre able to rank the most important texts for a given topic \cite{wissner2006preparation}. We are also able to explore interactions between world cultures by ranking articles to find the top 100 historical figures and which cultures have the largest reach, for example a culture will have a high PageRank if it has many inlinks from other cultures. PageRank reveals that Carl Linnaeus is the most `important' page due to the fact that he developed the scientific classification system we use for animals, insects and plants, and so these pages link back to him \cite{eom2015interactions}. We are also able to conclude that the most important figures in history are Western men who are born after the 17th Century. This mathematical analysis of historical figures and cultures using Wikipedia can be very useful in understanding world history.

\section{Chemistry} \label{sec:chem}
The PageRank algorithm has been used when studying molecules in chemistry, specifically in MoleculaRnetworks \cite{JCC:JCC22917}, which is a toolkit for identifying a shape of a solvent and exploring the H-bonding in that solvent. We are able to assess changes to the Hydrogen bond network due to the solute through the analysis of the network structure, as a graph is formed where the nodes are water molecules, and links represent potential hydrogen bonds, as shown in Figure \ref{fig:chem}. 

\begin{figure}[h]
\includegraphics[width=\linewidth]{Decomposition_of_simulation_date_into_a_graph_-chem.jpg}
\caption{Decomposition of simulation data into a graph, taken from \cite{JCC:JCC22917}}
\label{fig:chem}
\end{figure}

PageRank of each water molecule is determined by the number of edges connected to it, along with the number of edges connected to the nodes neighbours. As each polyhedron has its own unique PageRank vector, we are able to access a database in order to confirm the polyhedra structure. The use of PageRank in this toolkit is a novel way to extract chemical information, and can enhance the traditional methods used in chemical spectroscopy.

\section{Recommender Systems} \label{sec:recommender}
PageRank can modified so that it can be used in recommender systems for companies such as Netflix or Amazon, this modified PageRank is ItemRank, as developed by Gori in 2007 \cite{gleich2015pagerank}. ItemRank makes personalised product suggestions by extracting knowledge for the users prior interactions. ItemRank is a random-walk based algorithm where users rate the items, and they wish to be recommended items that have been rated highly, and has been shown to perform better than over algorithms whilst being less complex \cite{gori2007itemrank}.

\section{Roads and Urban Networks} \label{sec:Roads}
We are able to evaluate urban spaces and road networks using the PageRank algorithm in order to predict traffic flow and also human movement. Jiang et al. found that Weighted PageRank, as discussed in Section \ref{sec:weighted} of this report, is the best metric with regards to correlating or predicting traffic flow \cite{1742-5468-2008-07-P07008}. An urban space can be topologically represented by a connectivity graph as shown in Figure \ref{fig:city rep},
\begin{figure}[h]
\centering
\includegraphics[width=\linewidth]{map_view.jpeg}
\caption{(a) A fictive urban system, (b) its axial map and (c) connectivity graph, taken from \cite{doi:10.1080/13658810802022822}}
\label{fig:city rep}
\end{figure}
and over 60\% of human movement has been shown to be predicted or explained using a connectivity graph \cite{doi:10.1080/13658810802022822}.



This application is slightly different to the usual PageRank algorithm, as the graph formed from a urban space is a connected graph, and so there are no dangling nodes involved, in terms of a random walker, they will never get stuck, and so we are able to use the power method on the hyperlink matrix formed rather than having to construct the Google matrix. We assume that the random walker will never get tired and that they always move to one of the connecting roads with non-uniform probability, as the walker will prefer streets which are well-connected in the road network. Due to this non-uniform probability, we need to use the Weighted PageRank algorithm as discussed in Section \ref{sec:weighted} as opposed to the standard algorithm.
\FloatBarrier
\subsection{Durham} \label{sec:durham}
Jiang showed that PageRank scores are significantly correlated to human movement in four areas of London \cite{doi:10.1080/13658810802022822}, and we will use this study as a basis for our analysis of vehicle movement in Durham. 
\subsubsection{Methodology}
In order to represent the road network in Durham, we first produce an axial map and then are able to form a connectivity graph. The axial map is produced by taking a map of Durham, Figure \ref{fig:durham map},\begin{figure}
\centering
\includegraphics[width=0.75\linewidth]{durham_with_colour.jpg}
\caption{Durham road network, adapted from \cite{undergraduate}}
\label{fig:durham map}
\end{figure} and then drawing a set of intersecting lines which represent the road network \cite{Axialmap40:online}. Thus we are able to form the axial map given in Figure \ref{fig:durham axial},
\begin{figure}
\centering
\includegraphics[width=0.75\linewidth]{axial_marked.png}
\caption{Durham road network represented by an axial graph}
\label{fig:durham axial}
\end{figure}
This axial map can now be represented as a directed hypergraph, Figure \ref{fig:durham graph},
\begin{figure}
\centering
\includegraphics[width=0.75\linewidth]{ipe_durham.png}
\caption{Durham road network represented by a hypergraph}
\label{fig:durham graph}
\end{figure}
where the axial lines represent the nodes, and intersections are the links. 

The data has been collected from noting the vehicle flow for 10 minutes at each of the observation sites, as indicated in Figure \ref{fig:durham axial}, and care has been taken to ensure that the data was collected between 12pm and 1pm over the course of several days. This precaution was taken in order to attempt to lessen any effects of `rush-hour' traffic.
\FloatBarrier
\subsubsection{Results and Discussion} \label{sec:results}

In Table \ref{tble:Durham comparison}, we have produced a ranking for each node in the road network given in Figure \ref{fig:durham graph} based on the PageRank and WPR vectors, which are further detailed in Figures \ref{fig:DH}, \ref{fig:DWH}, \ref{fig:DPR} and Table \ref{tble:Durham cars} in Appendix \ref{app:results}. 
\begin{table} \caption{Comparison of PageRankings produced by standard algorithm and the Weighted PageRank algorithm on Durham}
 \centering
 \begin{tabular} {r l| c c| c} 
 \multicolumn{2}{c|}{Node}& PageRank & Weighted PageRank & Car Rank\\ [0.5ex] 
 \hline
 1&A390&1&1&1\\
 \rowcolor{yellow}
 2&North Rd&8&8&18\\
 3&Neville St&19&22&14\\
 4&Allergate&22&19&22\\
 5&Crossgate&24&24&13\\
 6&Margery Ln&6&11&10\\
 7&Grove St&23&23&23\\
 8&Quarryheads Ln&11&6&9\\
 9&Potters Bank&13&2&5\\
 10&Elvet Hill Rd&7&17&15\\
 \rowcolor{yellow}
 11&South Rd&5&18&2\\
  \rowcolor{yellow}
 12&Milburngate&20&25&24\\
 13&South St&25&13&20\\
  \rowcolor{yellow}
 14&Silver St&2&14&19\\
 15&Claypath&9&4&12\\
 16&Saddler St&10&7&16\\
  \rowcolor{yellow}
 17&Bailey&4&5&21\\
  \rowcolor{yellow}
 18&Elvet Bridge&16&20&25\\
 19&New Elvet&14&5&4\\
 20&Old Elvet&12&9&8\\
 21&Court Ln&15&12&17\\
 22&Church St&21&10&7\\
  \rowcolor{yellow}
 23&Hallgarth St&17&21&6\\
  \rowcolor{yellow}
 24&Stockton Rd&18&16&3\\
 25&Whinney Hill&3&3&11\\
  \end{tabular}
 \label{tble:Durham comparison}
\end{table}
We are able to conclude that PR and WPR are weakly correlated with vehicle movement in Durham, as given in Table \ref{tble:Correlation}, which is unexpected, as WPR and PR have been shown to be strong indicators of vehicle flow in built up urban spaces.

\begin{table}\caption{R-square correlation between PR, WPR and Human Movement}
\centering
\begin{tabular}{r| c|c}
 & All Nodes& Subset of Nodes \\
 \hline
 WPR & 0.1230 & 0.3550\\
 PR & 0.0780 & 0.3043\\ 
\end{tabular}
\label{tble:Correlation}
\end{table}

We are able to justify our findings by considering the fact that Durham is a very pedestrianised area. Hence there is less vehicle movement in certain areas of the town, for example no cars were observed at node 18, Elvet Bridge, as even though the road can be used by vehicles, it is used very infrequently due to the large pedestrian flow, and so vehicles are more likely to use other roads to reach their destination. 

We can also note that certain nodes contain links that are not represented in the road network used in this study, for example node 24, Stockton Road, is a major road which links Durham to Middlesbrough, and so instead of being ranked low as predicted by the WPR algorithm, instead has a high ranking, as shown in Table \ref{tble:Durham comparison}. This argument also applies to node 11, South Rd, which connects the road network to other cities in Durham county such as Bishop Auckland, node 23 Hallgarth St and Whinney Hill, also ranks highly due to linking Durham city with Middlesbrough. 	

It is worth noting that road work is currently occurring at some of observation sites at the nodes 2, 12 and 14, North Rd, Milburngate and Silver St. This road work has impacted the accessibility of these nodes.

If we remove these problematic nodes from the data set, as indicated by the highlighted nodes in Table \ref{tble:Durham comparison}, we are able to produce a slightly stronger correlation, as shown in Table \ref{tble:Correlation}.

The dataset used in this study is limited, as only a small proportion of total vehicle movement was recorded for each node, also the data was collected in a very traditional way, by manually counting the number of cars that passed an observation spot. This could be further improved in the future by use of more accurate data collection of human movement, for example the Department for Transport is currently able to monitor road use by the use of automatic traffic counters \cite{transport}. It is possible that as Durham is a very pedestrianised city, the pedestrian movement might be correlated to WPR as indicated by Jiang's study, and so this study could be further developed by using pedestrian movement as the dataset in the future.


%--------------------Conclusion
\chapter{Conclusion} \label{chap:Conclusion}

%--------------------Bibliography
\newpage
\addcontentsline{toc}{chapter}{References}
\bibliographystyle{plain}
\bibliography{bibliography.bib}
%--------------------Appendices
\begin{appendices}
\chapter{Google Matrix}
\begin{align*}
\textbf{G} &  = 0.85\cdot\textbf{H} + (0.85\cdot\textbf{a} + 0.15)\left(\frac{1}{n}\cdot 1^T\right)\\
& = 0.85\cdot\textbf{H} + \left((0.85\left(\begin{array}{c}
0\\0\\0\\0\\0\\0\\1\\0\end{array}\right) + 0.15\left(\begin{array}{c}
1\\1\\1\\1\\1\\1\\1\\1\end{array}\right)\right)\left(\frac{1}{8}\cdot\left(\begin{array}{cccccccc}
1&1&1&1&1&1&1&1\end{array}\right)\right)\\
&= 0.85\left(
\begin{array}{cccccccc}
0 & 0.28\overline{3} & 0 & 0.28\overline{3} & 0 & 0.28\overline{3} & 0 & 0  \\
0 & 0 & 0.425 & 0 & 0.425 & 0 & 0 & 0  \\
0 & 0 & 0 & 0 & 0.850 & 0 & 0 & 0  \\
0 & 0 & 0 & 0 & 0.425 & 0 & 0.425 & 0  \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0.850  \\
0 & 0 & 0 & 0.28\overline{3} & 0 & 0 & 0.28\overline{3} & 0.28\overline{3}  \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0  \\
0 & 0 & 0 & 0 & 0.425 & 0 & 0.425 & 0  \\
\end{array}
\right) \\&+ \left(
\begin{array}{cccccccc}
0.019 & 0.019 & 0.019 & 0.019 & 0.019 & 0.019 & 0.019 & 0.019  \\
0.019 & 0.019 & 0.019 & 0.019 & 0.019 & 0.019 & 0.019 & 0.019  \\
0.019 & 0.019 & 0.019 & 0.019 & 0.019 & 0.019 & 0.019 & 0.019  \\
0.019 & 0.019 & 0.019 & 0.019 & 0.019 & 0.019 & 0.019 & 0.019  \\
0.019 & 0.019 & 0.019 & 0.019 & 0.019 & 0.019 & 0.019 & 0.019  \\
0.019 & 0.019 & 0.019 & 0.019 & 0.019 & 0.019 & 0.019 & 0.019  \\
0.125 & 0.125 & 0.125 & 0.125 & 0.125 & 0.125 & 0.125 & 0.125  \\
0.019 & 0.019 & 0.019 & 0.019 & 0.019 & 0.019 & 0.019 & 0.019 
\end{array}
\right)\\
&= \left(
\begin{array}{cccccccc}
0.019 & 0.302 & 0.019 & 0.302 & 0.019 & 0.302 & 0.019 & 0.019  \\
0.019 & 0.019 & 0.444 & 0.019 & 0.444 & 0.019 & 0.019 & 0.019  \\
0.019 & 0.019 & 0.019 & 0.019 & 0.869 & 0.019 & 0.019 & 0.019  \\
0.019 & 0.019 & 0.019 & 0.019 & 0.444 & 0.019 & 0.444 & 0.019  \\
0.019 & 0.019 & 0.019 & 0.019 & 0.019 & 0.019 & 0.019 & 0.869  \\
0.019 & 0.019 & 0.019 & 0.302 & 0.019 & 0.019 & 0.302 & 0.302  \\
0.125 & 0.125 & 0.125 & 0.125 & 0.125 & 0.125 & 0.125 & 0.125  \\
0.019 & 0.019 & 0.019 & 0.019 & 0.444 & 0.019 & 0.444 & 0.019 
\end{array}
\right)
\end{align*} 
\chapter{Durham Results} \label{app:results}
\begin{figure} [h!]  
\begin{equation*} \renewcommand*{\arraystretch}{1.05}
\left(
\begin{array}{ccccccccccccccccccccccccc}
0&\frac{1}{8}&0&\frac{1}{8}&\frac{1}{8}&\frac{1}{8}&0&0&0&0&0&\frac{1}{8}&0&\frac{1}{8}&\frac{1}{8}&0&0&0&\frac{1}{8}&0&0&0&0&0&0\\

\frac{1}{4}&0&\frac{1}{4}&0&\frac{1}{4}&0&0&0&0&0&0&\frac{1}{4}&0&0&0&0&0&0&0&0&0&0&0&0&0\\

0&0&0&1&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0\\

\frac{1}{2}&0&0&0&\frac{1}{2}&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0\\

0&0&0&0&0&\frac{1}{2}&0&0&0&0&0&0&\frac{1}{2}&0&0&0&0&0&0&0&0&0&0&0&0\\

\frac{1}{3}&0&0&0&0&0&\frac{1}{3}&\frac{1}{3}&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0\\

0&0&0&0&0&\frac{1}{3}&0&\frac{1}{3}&0&0&0&0&\frac{1}{3}&0&0&0&0&0&0&0&0&0&0&0&0\\

0&0&0&0&0&\frac{1}{7}&\frac{1}{7}&0&\frac{1}{7}&0&\frac{1}{7}&0&\frac{1}{7}&0&0&0&0&0&0&0&0&\frac{1}{7}&0&\frac{1}{7}&0\\

0&0&0&0&0&0&0&\frac{1}{2}&0&\frac{1}{2}&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0\\

0&0&0&0&0&0&0&0&\frac{1}{2}&0&\frac{1}{2}&0&0&0&0&0&0&0&0&0&0&0&0&0&0\\

0&0&0&0&0&0&0&\frac{1}{4}&0&\frac{1}{4}&0&0&0&0&0&0&0&0&0&0&0&\frac{1}{4}&0&\frac{1}{4}&0\\

1&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0\\

0&\frac{1}{4}&0&0&\frac{1}{4}&0&\frac{1}{4}&\frac{1}{4}&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0\\

\frac{1}{4}&\frac{1}{4}&0&0&0&0&0&0&0&0&0&0&0&0&\frac{1}{4}&\frac{1}{4}&0&0&0&0&0&0&0&0&0\\

\frac{1}{2}&0&0&0&0&0&0&0&0&0&0&0&0&\frac{1}{2}&0&0&0&0&0&0&0&0&0&0&0\\

0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&\frac{1}{2}&\frac{1}{2}&0&0&0&0&0&0&0\\

0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&1&0&0&0&0&0&0&0&0&0\\

0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&1&0&0&0&0&0&0\\

\frac{1}{4}&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&\frac{1}{4}&0&\frac{1}{4}&\frac{1}{4}&0&0\\

0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&\frac{1}{3}&0&\frac{1}{3}&0&0&0&\frac{1}{3}\\

0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&\frac{1}{2}&\frac{1}{2}&0&0&0&0&0\\

0&0&0&0&0&0&0&\frac{1}{5}&0&0&\frac{1}{5}&0&0&0&0&0&0&0&\frac{1}{5}&0&0&0&\frac{1}{5}&\frac{1}{5}&0\\

0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&\frac{1}{4}&0&0&\frac{1}{4}&0&\frac{1}{4}&\frac{1}{4}\\

0&0&0&0&0&0&0&\frac{1}{5}&0&0&\frac{1}{5}&0&0&0&0&0&0&0&0&0&0&\frac{1}{5}&\frac{1}{5}&0&\frac{1}{5}\\

0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&\frac{1}{3}&0&0&\frac{1}{3}&\frac{1}{3}&0\\

\end{array}
\right)
\end{equation*} 
\caption{Hyperlink matrix for Durham} \label{fig:DH}
\end{figure}  

\begin{landscape}
\begin{figure} [h!] 
\begin{equation*} \renewcommand*{\arraystretch}{1.25}
\left(
\begin{array}{ccccccccccccccccccccccccc}
\frac{53}{92}&\frac{1}{42}&\frac{1}{84}&\frac{1}{42}&\frac{1}{53}&0&\frac{2}{63}&\frac{2}{27}&0&0&0&\frac{1}{84}&0&\frac{2}{63}&\frac{1}{84}&\frac{1}{84}&0&0&0&\frac{3}{70}&0&\frac{1}{14}&\frac{2}{35}&0&0\\

\frac{1}{12}&\frac{4}{19}&0&\frac{1}{19}&\frac{8}{95}&\frac{7}{88}&\frac{3}{64}&\frac{7}{64}&0&0&0&\frac{1}{38}&0&\frac{7}{66}&\frac{2}{27}&\frac{1}{48}&0&0&\frac{7}{66}&0&0&0&0&0&0\\

0&0&0&1&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0\\

0&\frac{7}{44}&0&\frac{9}{44}&\frac{7}{88}&0&\frac{8}{67}&0&0&0&0&\frac{1}{25}&0&\frac{7}{44}&\frac{7}{88}&0&0&0&\frac{7}{44}&0&0&0&0&0&0\\

\frac{6}{25}&\frac{12}{89}&\frac{1}{60}&\frac{5}{66}&\frac{8}{85}&\frac{3}{47}&\frac{3}{80}&\frac{7}{80}&0&0&0&\frac{3}{79}&0&\frac{4}{47}&\frac{2}{47}&0&0&0&\frac{4}{47}&0&0&0&0&0&0\\

0&\frac{2}{33}&0&\frac{1}{33}&\frac{1}{33}&\frac{19}{97}&\frac{1}{26}&\frac{1}{14}&\frac{1}{39}&0&\frac{2}{39}&\frac{1}{66}&\frac{1}{5}&\frac{2}{33}&\frac{1}{33}&0&0&0&\frac{2}{33}&0&0&\frac{5}{78}&0&\frac{5}{78}&0\\

\frac{8}{63}&\frac{3}{56}&0&0&\frac{2}{75}&\frac{3}{52}&\frac{8}{55}&\frac{17}{83}&\frac{1}{26}&0&\frac{1}{13}&0&\frac{1}{13}&0&0&0&0&0&0&0&0&\frac{5}{52}&0&\frac{5}{52}&0\\

\frac{5}{73}&\frac{1}{35}&0&0&\frac{1}{70}&\frac{2}{81}&\frac{1}{21}&\frac{21}{52}&0&\frac{1}{30}&\frac{3}{44}&0&\frac{2}{61}&0&0&0&0&0&\frac{3}{86}&0&0&\frac{6}{73}&\frac{3}{44}&\frac{3}{74}&\frac{2}{39}\\

0&0&0&0&0&\frac{7}{78}&\frac{7}{78}&0&\frac{2}{15}&0&\frac{19}{71}&0&\frac{3}{25}&0&0&0&0&0&0&0&0&\frac{3}{20}&0&\frac{3}{20}&0\\

0&0&0&0&0&0&0&\frac{1}{2}&0&\frac{1}{7}&0&0&0&0&0&0&0&0&0&0&0&\frac{10}{57}&0&\frac{10}{57}&0\\

0&0&0&0&0&\frac{2}{47}&\frac{2}{47}&\frac{9}{55}&\frac{5}{79}&0&\frac{11}{50}&0&\frac{3}{53}&0&0&0&0&0&\frac{1}{21}&0&0&\frac{11}{86}&\frac{3}{32}&\frac{7}{99}&\frac{4}{57}\\

\frac{1}{5}&\frac{7}{55}&\frac{1}{40}&\frac{5}{44}&\frac{4}{63}&\frac{2}{21}&0&0&0&0&0&\frac{5}{88}&0&\frac{7}{55}&\frac{4}{63}&0&0&0&\frac{7}{55}&0&0&0&0&0&0\\

0&0&0&0&0&\frac{19}{84}&\frac{3}{52}&\frac{3}{28}&\frac{1}{26}&0&\frac{1}{13}&0&\frac{19}{63}&0&0&0&0&0&0&0&0&\frac{5}{52}&0&\frac{5}{52}&0\\

\frac{4}{27}&\frac{14}{99}&0&\frac{7}{99}&\frac{7}{99}&\frac{7}{55}&0&0&0&0&0&\frac{3}{85}&0&\frac{14}{65}&\frac{7}{99}&0&0&0&\frac{14}{99}&0&0&0&0&0&0\\

\frac{1}{9}&\frac{13}{66}&0&\frac{7}{99}&\frac{7}{99}&\frac{7}{55}&0&0&0&0&0&\frac{3}{85}&0&\frac{14}{99}&\frac{6}{61}&\frac{1}{36}&0&0&\frac{14}{99}&0&0&0&0&0&0\\

\frac{1}{2}&\frac{1}{4}&0&0&0&0&0&0&0&0&0&0&0&0&\frac{1}{8}&\frac{1}{8}&0&0&0&0&0&0&0&0&0\\

0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&\frac{1}{2}&\frac{1}{2}&0&0&0&0&0&0&0\\

0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&\frac{1}{2}&\frac{1}{2}&0&0&0&0&0&0&0\\

0&\frac{2}{33}&0&\frac{1}{33}&\frac{1}{33}&\frac{1}{22}&0&\frac{5}{66}&0&0&\frac{1}{23}&\frac{1}{66}&0&\frac{2}{33}&\frac{1}{33}&0&0&0&\frac{23}{80}&\frac{2}{97}&\frac{2}{63}&\frac{1}{18}&\frac{1}{23}&\frac{1}{18}&\frac{5}{44}\\

\frac{6}{25}&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&\frac{2}{35}&\frac{16}{77}&0&\frac{3}{20}&\frac{11}{50}&\frac{1}{8}&0\\

0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&\frac{4}{9}&0&\frac{2}{9}&0&0&0&\frac{1}{3}\\

\frac{6}{65}&0&0&0&0&\frac{1}{32}&\frac{1}{32}&\frac{3}{26}&\frac{1}{48}&\frac{1}{62}&\frac{3}{40}&0&\frac{1}{24}&0&0&0&0&0&\frac{2}{55}&\frac{1}{29}&0&\frac{23}{97}&\frac{7}{88}&\frac{11}{80}&\frac{1}{19}\\

\frac{11}{87}&0&0&0&0&0&0&\frac{9}{55}&0&0&\frac{3}{32}&0&0&0&0&0&0&0&\frac{1}{21}&\frac{2}{23}&0&\frac{3}{22}&\frac{9}{43}&\frac{5}{76}&\frac{4}{57}\\

0&0&0&0&0&\frac{2}{57}&\frac{2}{57}&\frac{2}{15}&\frac{1}{43}&\frac{1}{55}&\frac{5}{58}&0&\frac{3}{64}&0&0&0&0&0&\frac{7}{87}&\frac{1}{31}&0&\frac{7}{45}&\frac{1}{12}&\frac{13}{62}&\frac{5}{83}\\

0&0&0&0&0&0&0&\frac{8}{63}&0&0&\frac{5}{69}&0&0&0&0&0&0&0&\frac{18}{95}&0&\frac{1}{18}&\frac{10}{53}&\frac{5}{69}&\frac{5}{51}&\frac{11}{56}\\

\end{array}
\right)
\end{equation*} 
\caption{Weighted hyperlink matrix for Durham} \label{fig:DWH}
\end{figure}  
\end{landscape}

\begin{figure} [H]  
\begin{equation*} 
\boldsymbol\pi_{PR} = \left(
\begin{array}{c}
0.0857\\
0.0300\\
0.0124\\
0.0256\\
0.0423\\
0.0557\\
0.0423\\
0.0867\\
0.0289\\
0.0289\\
0.0499\\
0.0215\\
0.0466\\
0.0237\\
0.0201\\
0.0252\\
0.0167\\
0.0167\\
0.0696\\
0.0389\\
0.0107\\
0.0634\\
0.0527\\
0.0601\\
0.0384
\end{array}
\right) \qquad 
\boldsymbol\pi_{WPR} = \left(
\begin{array}{c}
0.1023 \\
0.0443 \\
0.0077 \\
0.0313 \\
0.0240 \\
0.0482 \\
0.0295 \\
0.1090 \\
0.0163 \\
0.0126 \\
0.0497 \\
0.0137 \\
0.0379 \\
0.0353 \\
0.0215 \\
0.0093 \\
0.0400 \\
0.0400 \\
0.0691 \\
0.0223 \\
0.0120 \\
0.0785 \\
0.0482 \\
0.0555 \\
0.0396 
\end{array}
\right)
\end{equation*} 
\caption{PageRank and Weighted PageRank vectors for Durham} \label{fig:DPR}
\end{figure}  
\begin{table}[h] \caption{Number of cars at each node}
\centering
 \begin{tabular} {r l| c |c} 
 \multicolumn{2}{l|}{Node} & Cars & Average \\ [0.5ex] 
 \hline
 \multirow{6}{*}{1}&\multirow{6}{*}{A390}&208&\multirow{6}{*}{205} \\
 & &193\\ 
 & &195\\
 & &180\\
 & &231\\
 & &220\\
  \hline
\multirow{3}{*}{2} &\multirow{3}{*}{North Rd}& 40 &\multirow{3}{*}{13}\\
  &&0\\
  &&0\\
  \hline
 3&Neville St&30&14\\
  \hline
 4&Allergate&4&4\\
  \hline
\multirow{2}{*}{5}&\multirow{2}{*}{Crossgate}&34&\multirow{2}{*}{31}\\
& &28\\ 
 \hline
 6 &Margery Ln&75&75\\
 \hline
 7 & Grove St&3&3\\
 \hline
 \multirow{3}{*}{8}&\multirow{3}{*}{Quarryheads Ln}&73&\multirow{3}{*}{73}\\
 &&76\\
 &&80\\
 \hline
 9 & Potters Bank&105&105\\
 \hline
 10 & Elvet Hill Rd&20&20\\
 \hline
 11 & South Rd&183&183\\
 \hline
 12 & Milburngate&1&1\\
 \hline
 \multirow{2}{*}{13}&\multirow{2}{*}{South St}&7&\multirow{2}{*}{8}\\
 &&9\\  
 \hline
 \multirow{2}{*}{14}&\multirow{2}{*}{Silver St}&11&\multirow{2}{*}{6}\\
 &&0\\
 \hline
 15& Claypath&44&44\\
 \hline
 \multirow{2}{*}{16}&\multirow{2}{*}{Saddler St}&19&\multirow{2}{*}{20}\\
 &&21\\
 \hline
 17& Bailey&7&7\\
 \hline
 18 & Elvet Bridge&0&0\\
 \hline
 \multirow{3}{*}{19}&\multirow{3}{*}{New Elvet}&170&\multirow{3}{*}{167}\\
 &&162\\
 &&178\\
 \hline
 20 & Old Elvet&79&79\\
 \hline
 21 & Court Ln&14&14\\
 \hline
 22 & Church St&85&85\\
 \hline
 23 &Hallgarth St&105&105\\
 \hline
 24 & Stockton Rd&178&178\\
 \hline
 25 & Whinney Hill&39&39\\ 
 \end{tabular}
 \label{tble:Durham cars}
\end{table}



\chapter{Matlab code} \label{app:code}

\begin{lstlisting} [caption=Matlab code for implementing power method taken from \cite{langville} ]
function [pi,time,numiter] = \hbox{PageRank}(pi0,H,n,alpha,epsilon);

% \hbox{PageRank} computes the \hbox{PageRank} vector for an n-by-n Markov
%           matrix H with starting vector pi0 (a row vector)
%           and scaling parameter alpha (scalar). Uses power method.
%
% EXAMPLE: [pi,time,numiter] = \hbox{PageRank}(pi0,H,1000,.9,1e-8)
%
% INPUT: pi0 = starting vector at iteration 0 (row vector)
%        H = row-normalized hyperlink matrix (n-by-n sparse matrix)
%        n = size of H matrix (scalar)
%        alpha = scaling parameter in \hbox{PageRank} model (scalar)
%        epsilon = convergence tolerance (scalar, e.g. 1e-8)
%
% OUTPUT: pi = \hbox{PageRank} vector
%         time = time required to compute \hbox{PageRank} vector
%         numiter = number of iterations until convergence
%        
% Starting vector is usually set to uniform vector,
% pi0=1/n*ones(1,n).
% NOTE: Matlab stores sparse matrices by columns, so it is faster
%       to do some operations on H', the transpose of H.
% get 'a', dangling node vector, where a(i) = 1, if node i 
%    is dangling node and 0 o.w.

rowsumvector = ones(1,n)*H';
nonzerorows=find(rowsumvector);
zerorows=setdiff(1:n,nonzerorows); l=length(zerorows);
a=sparse(zerorows,ones(1,1),ones(1,1),n,1);

k=0;
residual=1;
pi=pi0;
tic;

while(residual>=epsilon)
     prevpi=pi;
     k=k+1;
     pi=alpha*pi*H + (alpha*(pi*a)+1-alpha)*((1/n)*ones(1,n));
     residual=norm(pi-prevpi,1);
end

numiter=k;
time=toc;

\end{lstlisting}

\begin{lstlisting} [caption=Matlab code for implementing HITS power method taken from \cite{langville}]
function [x,y,time,numiter] = hits[L,x0,n,epsilon] 

% HITS computes the HITS authority vector x and hub vector y 
%      for an n-by-n adjacency matrix L with starting vector
%      x0 (a row vector). Uses power method on L'*L.
%
% EXAMPLE: [x,y,time,numiter] = hits[L,x0,100,1e-8]
%
% INPUT: L = adjacency matrix (n-by-n sparse matrix)
%        x0 = starting vector (row vector)
%        n = size of L matrix (integer)
%        epsilon = convergence tolerance (scalar, e.g. 1e-8)
%
% OUTPUT: x = HITS authority vector
%         y = HITS hub vector
%         time = time until convergence
%         numiter = number of iterations until convergence
%        
% The starting vector is usually set to the uniform vector,
% x0=1/n*ones(1/n)

k=0;
residual=1;
x=x0;
tic;

while(residual>=epsilon)
     prevx=x;
     k=k+1;
     x=x*L';
     x=x*L;
     x=x/sum(x);
     residual=norm(x-prevx,1);
end
y=x*L';
y=y/sum(y);
numiter=k;
time=toc;

\end{lstlisting}
\end{appendices}
\end{document}